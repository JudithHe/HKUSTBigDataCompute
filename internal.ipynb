{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multi-threading to submit jobs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 211:>  (0 + 5) / 5][Stage 212:>  (0 + 5) / 5][Stage 213:>  (0 + 5) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2 reports: Pi is roughly 3.1419104\n",
      "Worker 0 reports: Pi is roughly 3.1421728\n",
      "Worker 1 reports: Pi is roughly 3.1424496\n",
      "Worker 3 reports: Pi is roughly 3.141776\n",
      "Worker 4 reports: Pi is roughly 3.1428528\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import random\n",
    "\n",
    "partitions = 5\n",
    "n = 500000 * partitions\n",
    "\n",
    "# use different seeds in different threads and different partitions\n",
    "# a bit ugly, since mapPartitionsWithIndex takes a function with only index\n",
    "# and it as parameters\n",
    "def f1(index, it):\n",
    "    random.seed(index + 987231)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "def f2(index, it):\n",
    "    random.seed(index + 987232)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "def f3(index, it):\n",
    "    random.seed(index + 987233)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "    \n",
    "def f4(index, it):\n",
    "    random.seed(index + 987234)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "    \n",
    "def f5(index, it):\n",
    "    random.seed(index + 987245)\n",
    "    for i in it:\n",
    "        x = random.random() * 2 - 1\n",
    "        y = random.random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "f = [f1, f2, f3, f4, f5]\n",
    "    \n",
    "# the function executed in each thread/job\n",
    "def dojob(i):\n",
    "    count = sc.parallelize(range(1, n + 1), partitions) \\\n",
    "              .mapPartitionsWithIndex(f[i]).reduce(lambda a,b: a+b)\n",
    "    print(\"Worker\", i, \"reports: Pi is roughly\", 4.0 * count / n)\n",
    "\n",
    "# create and execute the threads\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=dojob, args=(i,))\n",
    "    threads += [t]\n",
    "    t.start()\n",
    "\n",
    "# wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finding Prime Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 97, 113, 193, 241, 257, 337, 353, 401, 433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 129:==========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = 500000\n",
    "allnumbers = sc.parallelize(range(2, n), 8).cache()\n",
    "composite = allnumbers.flatMap(lambda x: range(x*2, n, x)).repartition(8)\n",
    "prime = allnumbers.subtract(composite)\n",
    "print(prime.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62499, 62500, 62500, 62500, 62499, 62500, 62500, 62500]\n",
      "[704805, 704790, 704800, 704800, 704800, 704799, 704800, 704816]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5169, 1, 5219, 0, 5206, 0, 5189, 0, 5165, 0, 5199, 0, 5191, 0, 5199]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 97, 113, 193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 19, 67, 83]\n",
      "[44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542]\n"
     ]
    }
   ],
   "source": [
    "# Find the number of elements in each parttion\n",
    "def partitionsize(it): \n",
    "    yield len(list(it))\n",
    "\n",
    "print(allnumbers.mapPartitions(partitionsize).collect())\n",
    "print(composite.mapPartitions(partitionsize).collect())\n",
    "print(prime.mapPartitions(partitionsize).collect())\n",
    "print(prime.glom().take(2)[1][0:4])\n",
    "print(prime.glom().take(3)[2][0:4])\n",
    "print(prime.glom().take(4)[3][0:4])\n",
    "print(composite.glom().take(1)[0][0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 2], [2, 8], [7, 3], [3, 4, 1]]\n",
      "[[], [1, 1, 2, 8], [1, 2, 7, 3, 3, 4, 1]]\n",
      "[[1, 1], [1, 2, 2, 8], [7, 3, 3, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# repartition vs coalesce\n",
    "\n",
    "data = [1, 1, 1, 2, 2, 8, 7, 3, 3, 4, 1, 2, 3]\n",
    "\n",
    "rdd1 = sc.parallelize(data1, 5)\n",
    "print(rdd1.glom().collect())\n",
    "\n",
    "# repartition can increase or decrease the level of parallelism in this RDD. \n",
    "# Internally, this uses a chunk-based shuffle to redistribute data. (chunk size seems to be 10)\n",
    "rdd2 = rdd1.repartition(3)\n",
    "print(rdd2.glom().collect())\n",
    "\n",
    "#If you are decreasing the number of partitions in this RDD, consider using coalesce, \n",
    "# which can avoid performing a shuffle.\n",
    "# coalesce merges adjacent partitions, so it cannot fix skew issues!\n",
    "rdd3 = rdd1.coalesce(3)\n",
    "print(rdd3.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "[[(8, 9), (8, 9)], [(1, 2), (96, 97)], [(240, 241), (400, 401)], [(1, 2), (800, 801), (4, 5), (12, 13)]]\n",
      "[[(8, 18), (96, 97), (240, 241), (400, 401), (800, 801), (4, 5), (12, 13)], [(1, 4)], [], []]\n",
      "<pyspark.rdd.Partitioner object at 0x7fe0e1453cd0>\n",
      "<function portable_hash at 0x7fe0e05ca160>\n",
      "[[(8, 19), (96, 98), (240, 242), (400, 402), (800, 802), (4, 6), (12, 14)], [(1, 5)], [], []]\n",
      "test None\n",
      "test <function portable_hash at 0x7fe0e05ca160>\n",
      "[[(1, 4), (4, 5), (8, 18)], [(12, 13), (96, 97)], [(240, 241), (400, 401)], [(800, 801)]]\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fe0e149e670>\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fe0e149e670>\n"
     ]
    }
   ],
   "source": [
    "data = [8, 8, 1, 96, 240, 400, 1, 800, 4, 12]\n",
    "rdd = sc.parallelize(zip(data, data),4)\n",
    "print(rdd.partitioner)\n",
    "rdd = rdd.map(lambda t: (t[0], t[1]+1))\n",
    "print(rdd.partitioner)\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "rdd = rdd.reduceByKey(lambda x,y: x+y)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner)\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "\n",
    "rdd1 = rdd.map(lambda x: (x[0], x[1]+1))\n",
    "print(rdd1.glom().collect())\n",
    "print(\"test\",rdd1.partitioner)\n",
    "\n",
    "rdd2 = rdd.mapValues(lambda x: x+1)\n",
    "print(\"test\",rdd2.partitioner.partitionFunc)\n",
    "\n",
    "rdd = rdd.sortByKey()\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "rdd3 = rdd.mapValues(lambda x: x+1)\n",
    "print(rdd3.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two RDDs with different number of partitions\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 4)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "\n",
    "# They are not co-partitioned because they have different numbers of partitions.\n",
    "a = a.reduceByKey(lambda x,y: x+y)#hashed partition\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "# To avoid a third shuffle, use the same partition number in the first two shuffles:\n",
    "a = a.reduceByKey(lambda x,y: x+y, 8)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)#narrow partition(copartition now)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[(240, 240), (400, 400), (1, 1), (800, 800), (4, 4), (12, 12)], [(1, 1), (96, 96)], [], [(8, 8), (8, 8)]]\n",
      "None\n",
      "[[(8, 8), (8, 8), (96, 96), (240, 240), (400, 400), (800, 800), (4, 4), (12, 12)], [(1, 1), (1, 1)], [], []]\n",
      "<pyspark.rdd.Partitioner object at 0x7f89e4216d68>\n",
      "<function portable_hash at 0x7f89d1732f28>\n"
     ]
    }
   ],
   "source": [
    "data = [8, 8, 1, 96, 240, 400, 1, 800, 4, 12]\n",
    "rdd = sc.parallelize(zip(data, data),4)\n",
    "print(rdd.partitioner)\n",
    "\n",
    "# repartition does a random reparitioning, resulting in no partitioner.\n",
    "rdd1 = rdd.repartition(4)\n",
    "print(rdd1.glom().collect())\n",
    "print(rdd1.partitioner)\n",
    "\n",
    "# partitionBy partitions data by hashing the key.\n",
    "# This can only be applied on (key, value) pairs\n",
    "rdd2 = rdd.partitionBy(4)\n",
    "print(rdd2.glom().collect())\n",
    "print(rdd2.partitioner)\n",
    "print(rdd2.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[625, 625, 625, 625, 625, 625, 625, 625]\n",
      "[2500, 0, 0, 0, 0, 0, 0, 0]\n",
      "[334, 334, 333, 333, 333, 333, 333, 167]\n",
      "[334, 334, 333, 333, 333, 333, 333, 167]\n",
      "<function f at 0x7f89e4212620>\n"
     ]
    }
   ],
   "source": [
    "def partitionsize(it): yield len(list(it))\n",
    "    \n",
    "n = 40000\n",
    "\n",
    "def f(x):\n",
    "    return x % 15\n",
    "\n",
    "data1 = list(range(0, n, 16)) + list(range(0, n, 16))\n",
    "data2 = range(0, n, 8)\n",
    "rdd1 = sc.parallelize(zip(data1, data2), 8)\n",
    "print(rdd1.mapPartitions(partitionsize).collect())\n",
    "rdd2 = rdd1.reduceByKey(lambda x,y: x+y)\n",
    "print(rdd2.mapPartitions(partitionsize).collect())\n",
    "rdd3 = rdd2.partitionBy(8, f)\n",
    "print(rdd3.mapPartitions(partitionsize).collect())\n",
    "rdd4 = rdd1.reduceByKey(lambda x,y: x+y, partitionFunc=f)\n",
    "print(rdd4.mapPartitions(partitionsize).collect())\n",
    "print(rdd4.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "<function portable_hash at 0x7f5620192f28>\n",
      "[(0, (0, 0)), (16, (16, 16)), (32, (32, 32)), (48, (48, 48))]\n",
      "None\n",
      "<function portable_hash at 0x7f5620192f28>\n",
      "<function portable_hash at 0x7f5620192f28>\n",
      "8\n",
      "<function portable_hash at 0x7f5620192f28>\n",
      "[(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))]\n",
      "None\n",
      "16\n",
      "<function portable_hash at 0x7f5620192f28>\n",
      "[(0, (0, 0)), (16, (16, 16)), (32, (32, 32)), (48, (48, 48))]\n"
     ]
    }
   ],
   "source": [
    "# Join two RDDs not co-partitioned\n",
    "# The resulting RDD has twice the partition number\n",
    "\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "# After a shuffling operation, the resulting RDD is hash partitioned\n",
    "print(a.partitioner)\n",
    "a = a.reduceByKey(lambda x,y: x+y)\n",
    "print(a.partitioner.partitionFunc)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "print(b.partitioner.partitionFunc)\n",
    "\n",
    "# Join two RDDs co-partitioned: no shuffle is needed and partition number is the same\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "# coalesce/repartition removes the partitioner.\n",
    "b = b.coalesce(8)\n",
    "print(b.partitioner)\n",
    "c = a.join(b)  # This join still requires a shuffle\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "1 <function portable_hash at 0x7fe0e05ca160>\n",
      "[(0, (0, 0)), (12, (12, 12)), (24, (24, 24)), (36, (36, 36))]\n",
      "<function portable_hash at 0x7fe0e05ca160>\n",
      "8\n",
      "<function portable_hash at 0x7fe0e05ca160>\n",
      "[(0, (0, 0)), (8, (8, 8)), (16, (16, 16)), (24, (24, 24))]\n"
     ]
    }
   ],
   "source": [
    "# Create two RDDs with different number of partitions\n",
    "a = sc.parallelize(zip(range(10000), range(10000)), 4)\n",
    "b = sc.parallelize(zip(range(10000), range(10000)), 8)\n",
    "\n",
    "# They are not co-partitioned because they have different numbers of partitions.\n",
    "a = a.reduceByKey(lambda x,y: x+y)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)\n",
    "print(c.getNumPartitions())\n",
    "print(\"1\",c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])\n",
    "\n",
    "new = c.reduceByKey(lambda x,y: x+y)\n",
    "print(new.partitioner.partitionFunc)\n",
    "\n",
    "\n",
    "# To avoid a third shuffle, use the same partition number in the first two shuffles:\n",
    "a = a.reduceByKey(lambda x,y: x+y, 8)\n",
    "b = b.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "c = a.join(b)#there is no shuffle\n",
    "print(c.getNumPartitions())\n",
    "print(c.partitioner.partitionFunc)\n",
    "print(c.glom().first()[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [Row(a=1, b=2)], [], [], [], [Row(a=2, b=1)], [], [], [], [], [Row(a=1, b=1)], [], [], [], [Row(a=1, b=3)], [], [], [], [Row(a=2, b=4)], [], [], [], [], [Row(a=3, b=4)], [], [], [], [Row(a=4, b=5)], [], [], [], [Row(a=4, b=2)], [], [], [], [], [Row(a=5, b=1)], [], [], [], [Row(a=2, b=5)], [], [], [], [Row(a=1, b=3)]]\n",
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  5|    1|\n",
      "|  1|    4|\n",
      "|  3|    1|\n",
      "|  2|    3|\n",
      "|  4|    2|\n",
      "+---+-----+\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)  # Default in Spark 2.x \n",
    "print(spark.conf.get('spark.sql.shuffle.partitions'))  # number of partitions in a shuffle, default is 200\n",
    "\n",
    "# spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)  # Default in Spark 3.x \n",
    "#  When this is set to True, Spark will coalesce contiguous shuffle partitions according to the target size\n",
    "# (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes, default is 64 MB), to avoid too many small tasks.\n",
    "#  But this may not always give you the best performance!  \n",
    "\n",
    "data1 = [1, 2, 1, 1, 2, 3, 4, 4, 5, 2, 1]\n",
    "data2 = [2, 1, 1, 3, 4, 4, 5, 2, 1, 5, 3]\n",
    "\n",
    "df1 = spark.createDataFrame(zip(data1, data2), ['a', 'b'])\n",
    "print(df1.rdd.getNumPartitions())\n",
    "print(df1.rdd.glom().collect())\n",
    "\n",
    "df2 = df1.groupBy('a').count()\n",
    "df2.show()\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=0, b=0), Row(a=1, b=1), Row(a=2, b=2)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Effects of coalescePartitions\n",
    "\n",
    "n = 1000000\n",
    "partitions = 40\n",
    "df = spark.range(n)\n",
    "df = df.select(df[0].alias('a'), df[0].alias('b')).cache()\n",
    "# Don't use range() in python\n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 174:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|      sum(c)|\n",
      "+------------+\n",
      "|499999500000|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 174:=========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True) # default is True\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", False)  # default is True in Spark 3.2\n",
    "# When true, Spark ignores the target size specified by spark.sql.adaptive.advisoryPartitionSizeInBytes \n",
    "# (default 64MB) when coalescing contiguous shuffle partitions, and only respect the minimum partition\n",
    "# size specified by spark.sql.adaptive.coalescePartitions.minPartitionSize (default 1MB), to maximize the \n",
    "# parallelism. This is to avoid performance regression when enabling adaptive query execution. \n",
    "# It's recommended to set this config to false and respect the target size specified by \n",
    "# spark.sql.adaptive.advisoryPartitionSizeInBytes.\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', partitions)  # number of partitions in a shuffle, default is 200\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df1 = df.groupBy(df[0]).count()\n",
    "print(df1.rdd.getNumPartitions())\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def f(x):\n",
    "    s = 0\n",
    "    for i in range(500):\n",
    "        s+=i\n",
    "    return x\n",
    "\n",
    "myf = udf(f, IntegerType())\n",
    "\n",
    "df2 = df1.select('*', myf(df1[0]).alias('c')).select(sum('c'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [Row(a=5, b=1), Row(a=2, b=1), Row(a=2, b=4), Row(a=2, b=5), Row(a=4, b=5), Row(a=4, b=2)], [Row(a=3, b=4)], [], [Row(a=1, b=2), Row(a=1, b=1), Row(a=1, b=3), Row(a=1, b=3)]]\n",
      "None\n",
      "+---+---+-----------+-------------+\n",
      "|  a|  b|    hash(a)|(hash(a) % 6)|\n",
      "+---+---+-----------+-------------+\n",
      "|  5|  1| 1607884268|            2|\n",
      "|  2|  1| -797927272|           -4|\n",
      "|  2|  4| -797927272|           -4|\n",
      "|  2|  5| -797927272|           -4|\n",
      "|  4|  5| 1344313940|            2|\n",
      "|  4|  2| 1344313940|            2|\n",
      "|  3|  4|  519220707|            3|\n",
      "|  1|  2|-1712319331|           -1|\n",
      "|  1|  1|-1712319331|           -1|\n",
      "|  1|  3|-1712319331|           -1|\n",
      "|  1|  3|-1712319331|           -1|\n",
      "+---+---+-----------+-------------+\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Hash partitioner in SparkSQL\n",
    "\n",
    "import pyspark.sql.functions\n",
    "\n",
    "df1 = df1.repartition(6, df1['a'])\n",
    "print(df1.rdd.glom().collect())\n",
    "print(df1.rdd.partitioner)  # This doesn't work for dataframes, as the RDD underlying a dataframe is virtual\n",
    "\n",
    "# SparkSQL uses MurmurHash to make generating adversarial data more difficult\n",
    "# Calling SparkSQL's hash function\n",
    "df1.select('*', pyspark.sql.functions.hash(df1['a']), pyspark.sql.functions.hash(df1['a']) % 6).show()\n",
    "\n",
    "# Calling Python's hash function\n",
    "print(hash(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a| a1| b1|\n",
      "+---+---+---+\n",
      "|  0|  0|  0|\n",
      "|  1|  1|  1|\n",
      "|  2|  2|  2|\n",
      "|  3|  3|  3|\n",
      "|  4|  4|  4|\n",
      "|  5|  5|  5|\n",
      "|  6|  6|  6|\n",
      "|  7|  7|  7|\n",
      "|  8|  8|  8|\n",
      "|  9|  9|  9|\n",
      "+---+---+---+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [a#0L, a1#1L, b1#5L]\n",
      "   +- SortMergeJoin [a#0L], [a#4L], Inner\n",
      "      :- Sort [a#0L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(a#0L, 200), ENSURE_REQUIREMENTS, [plan_id=140]\n",
      "      :     +- Filter isnotnull(a#0L)\n",
      "      :        +- Scan ExistingRDD[a#0L,a1#1L]\n",
      "      +- Sort [a#4L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(a#4L, 200), ENSURE_REQUIREMENTS, [plan_id=141]\n",
      "            +- Filter isnotnull(a#4L)\n",
      "               +- Scan ExistingRDD[a#4L,b1#5L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join hints\n",
    "\n",
    "a = spark.createDataFrame(zip(range(10), range(10)), ['a', 'a1'])\n",
    "b = spark.createDataFrame(zip(range(10), range(10)), ['a', 'b1'])\n",
    "\n",
    "c = a.join(b, 'a')\n",
    "#c = a.join(b.hint('broadcast'), 'a')\n",
    "#c = a.join(b.hint('shuffle_hash'), 'a')\n",
    "#c = a.join(b.hint('merge'), 'a')\n",
    "c.show()\n",
    "\n",
    "c.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'explain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/ipykernel_7096/3850497605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrdd3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.mapValues(...).reduceByKey(...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrdd3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'explain'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 00:00:03 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 3952763 ms exceeds timeout 120000 ms\n",
      "23/04/01 00:00:03 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "[[(8, 9), (8, 9)], [(1, 2), (96, 97)], [(240, 241), (400, 401)], [(1, 2), (800, 801), (4, 5), (12, 13)]]\n",
      "[[(8, 18), (96, 97), (240, 241), (400, 401), (800, 801), (4, 5), (12, 13)], [(1, 4)], [], []]\n",
      "<pyspark.rdd.Partitioner object at 0x7fe0e152b9d0>\n",
      "<function portable_hash at 0x7fe0e05ca160>\n",
      "[[(8, 19), (96, 98), (240, 242), (400, 402), (800, 802), (4, 6), (12, 14)], [(1, 5)], [], []]\n",
      "None\n",
      "1, <function portable_hash at 0x7fe0e05ca160>\n",
      "[[(1, 4), (4, 5), (8, 18)], [(12, 13), (96, 97)], [(240, 241), (400, 401)], [(800, 801)]]\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fe0e142d040>\n",
      "<function RDD.sortByKey.<locals>.rangePartitioner at 0x7fe0e142d040>\n"
     ]
    }
   ],
   "source": [
    "data = [8, 8, 1, 96, 240, 400, 1, 800, 4, 12]\n",
    "rdd = sc.parallelize(zip(data, data),4)\n",
    "print(rdd.partitioner)\n",
    "rdd = rdd.map(lambda t: (t[0], t[1]+1))\n",
    "print(rdd.partitioner)\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "rdd = rdd.reduceByKey(lambda x,y: x+y)#trigger shuffle tuple by hashing the key\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner)\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "\n",
    "rdd1 = rdd.map(lambda x: (x[0], x[1]+1))#shallow map(key may be different) partition won't be maintained\n",
    "print(rdd1.glom().collect())\n",
    "print(rdd1.partitioner)\n",
    "\n",
    "rdd2 = rdd.mapValues(lambda x: x+1)#not changing the key; partition will be retained\n",
    "print(\"1,\",rdd2.partitioner.partitionFunc)#has the same partition func\n",
    "\n",
    "rdd = rdd.sortByKey()\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.partitioner.partitionFunc)\n",
    "rdd3 = rdd.mapValues(lambda x: x+1)\n",
    "print(rdd3.partitioner.partitionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
