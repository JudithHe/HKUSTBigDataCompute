{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "693fbc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb45996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02265840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      Country|\n",
      "+-------------+\n",
      "|       Sweden|\n",
      "|       Jersey|\n",
      "|     Malaysia|\n",
      "|       Turkey|\n",
      "|      Germany|\n",
      "|       France|\n",
      "|      Belgium|\n",
      "|      Finland|\n",
      "|United States|\n",
      "|        India|\n",
      "|       Kuwait|\n",
      "|        Malta|\n",
      "|        Italy|\n",
      "|       Norway|\n",
      "|        Spain|\n",
      "|      Denmark|\n",
      "|      Ireland|\n",
      "|       Israel|\n",
      "|      Iceland|\n",
      "|  South Korea|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e483196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01ca66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+\n",
      "|   Name|Price|country|\n",
      "+-------+-----+-------+\n",
      "|Joachim| 1200| Brazil|\n",
      "|  Diana| 7500| Brazil|\n",
      "+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\",\"Price\").filter(\"Country=='Brazil'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43b954de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_date: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Account_Created: string (nullable = true)\n",
      " |-- Last_Login: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d040a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|      Country|TotalPrice|\n",
      "+-------------+----------+\n",
      "|       Sweden|      8400|\n",
      "|       Jersey|      1200|\n",
      "|     Malaysia|      1200|\n",
      "|       Turkey|      2400|\n",
      "|      Germany|     22800|\n",
      "|       France|     30300|\n",
      "|      Belgium|      3600|\n",
      "|      Finland|      1200|\n",
      "|United States|    350350|\n",
      "|        India|      2400|\n",
      "|       Kuwait|      1200|\n",
      "|        Malta|      3600|\n",
      "|        Italy|      2400|\n",
      "|       Norway|     12000|\n",
      "|        Spain|      2400|\n",
      "|      Denmark|      8400|\n",
      "|      Ireland|     29100|\n",
      "|       Israel|      1200|\n",
      "|      Iceland|      1200|\n",
      "|  South Korea|      1200|\n",
      "+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").agg(sum(\"Price\")).withColumnRenamed('sum(Price)', 'TotalPrice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44e271e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.groupBy(\"Country\").agg(sum(\"Price\")).withColumnRenamed('sum(Price)', 'TotalPrice').\\\n",
    "orderBy('TotalPrice', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bc8ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"countries.csv\",header=True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b2a6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bceaea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID|TotalPrice|\n",
      "+---+----------+\n",
      "|  2|    350350|\n",
      "|  1|     63600|\n",
      "|  8|     42000|\n",
      "|  5|     30300|\n",
      "|  7|     29100|\n",
      "|  3|     22800|\n",
      "| 19|     22800|\n",
      "| 12|     19200|\n",
      "|  6|     14400|\n",
      "| 16|     12000|\n",
      "| 33|      8700|\n",
      "| 15|      8400|\n",
      "| 13|      8400|\n",
      "| 26|      3600|\n",
      "| 22|      3600|\n",
      "| 28|      3600|\n",
      "| 14|      3600|\n",
      "| 10|      3600|\n",
      "| 24|      2400|\n",
      "|  9|      2400|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").agg(sum(\"Price\")).withColumnRenamed('sum(Price)', 'TotalPrice')\\\n",
    ".join(df2,\"Country\").select(\"ID\",\"TotalPrice\")\\\n",
    ".orderBy('TotalPrice', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7f9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|dst|contri|\n",
      "+---+------+\n",
      "|  4|   1.0|\n",
      "|  3|   0.5|\n",
      "|  2|   0.5|\n",
      "|  1|   1.0|\n",
      "|  1|   0.5|\n",
      "|  3|   0.5|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 1\n",
    "\n",
    "lines = spark.read.text(\"pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "contribs = links.join(ranks,'src').groupBy('src').count().join(links,'src').join(ranks,'src')\\\n",
    "        .select('dst',(col('rank')/col('count')).alias('contri')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eec0495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 434:==============>                                          (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  src|              rank|\n",
      "+-----+------------------+\n",
      "|    3| 16775.73526883919|\n",
      "|    2|15259.179382798762|\n",
      "|    1|10159.700782343525|\n",
      "|    4|  8992.66455229212|\n",
      "|    5| 7293.343859690132|\n",
      "|    7| 6000.921547713604|\n",
      "|    6| 5057.615774722168|\n",
      "|    8| 2593.763706927988|\n",
      "|    9|  2394.02981928632|\n",
      "|   10| 2283.782476601727|\n",
      "|   13|1862.5063389672428|\n",
      "|   14| 1822.667679424879|\n",
      "|14175|1289.2149600703403|\n",
      "|  117|1095.6979368422024|\n",
      "|  120| 1088.077815887215|\n",
      "|   11|1076.9015952513955|\n",
      "|   16| 994.3828772892236|\n",
      "|  122| 975.0243284791197|\n",
      "|   15| 966.2459632448032|\n",
      "|14173| 939.6263446314646|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "#lines = spark.read.text(\"pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "outdegrees = links.groupBy('src').count()\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank'))\n",
    "for iteration in range(numOfIterations):\n",
    "# FILL IN THIS PART\n",
    "    contribs = outdegrees.join(links,'src').join(ranks,'src')\\\n",
    "        .select('dst',(col('rank')/col('count')).alias('contri'))\n",
    "    ranks = contribs.groupBy('dst').agg(sum(\"contri\").alias('Totalcontri'))\\\n",
    "        .select(col('dst').alias('src'),(0.85*col('Totalcontri')+0.15).alias('rank'))\n",
    "    \n",
    "    \n",
    "ranks.orderBy(desc('rank')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674addd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f6b3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 343:>                                                        (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 16775.735268839184), ('2', 15259.179382798724), ('1', 10159.700782343547), ('4', 8992.664552292099), ('5', 7293.343859690148)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 343:==================================>                      (3 + 2) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    #each destination in urls equally share the page rank\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)#yield is used in for-loop and writes a new element into the resulting sequence.\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL(source)         neighbor URL(destination)\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "#lines = sc.textFile(\"pagerank_data.txt\", 1)\n",
    "lines = sc.textFile(\"dblp.in\", 5)\n",
    "#print(lines.take(10))\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0)#do mapping only for value in the list of (key,value) pairs\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    \n",
    "    contribs = links.join(ranks)  \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls(destinations), rank))\n",
    "    # join: Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "    \n",
    "    #contribs is of form:[(u1,1/3),(u2,1/3),...,...] key: destincation url, value: contribution to this destination\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank*0.85+0.15 )\n",
    "    # ranks = contribs.reduceByKey(add).map(lambda t: (t[0], t[1] * 0.85 + 0.15))\n",
    "\n",
    "print(ranks.top(5, lambda x: x[1]))#only one action ; for loop only construct the lineage graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3632b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
