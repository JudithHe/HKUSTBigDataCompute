{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive.  All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/\n",
    "For example, https://www.cse.ust.hk/msbd5003/data/fruits.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    }
   ],
   "source": [
    "# Read data from local file system:\n",
    "print(sc.version)\n",
    "\n",
    "fruits = sc.textFile('fruits.txt') #\n",
    "yellowThings = sc.textFile('yellowthings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'canary melon', 'grap', 'lemon', 'orange', 'pineapple', 'strawberry']\n",
      "['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'banana', 'canary melon', 'grap', 'lemon'], ['orange', 'pineapple', 'strawberry']]\n",
      "[['banana', 'bee', 'butter', 'canary melon', 'gold'], ['lemon', 'pineapple', 'sunflower']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(fruits.collect())\n",
    "print(yellowThings.collect())\n",
    "\n",
    "print(fruits.getNumPartitions())\n",
    "\n",
    "print(fruits.glom().collect())\n",
    "print(yellowThings.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from HDFS :\n",
    "fruits = sc.textFile('hdfs://url:9000/pathname/fruits.txt')\n",
    "fruits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "##  RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruitsReversed.cache()\n",
    "# try changing the file and re-execute with and without cache\n",
    "fruitsReversed.collect()\n",
    "# What happens when you uncomment the first line and run the whole program again with cache()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "k = 5\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= k)\n",
    "print(shortFruits.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap; flatmap like list extension; map like list append\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "print(characters.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "print(fruitsAndYellowThings.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "print(yellowFruits.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.subtract(yellowThings).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "print(distinctFruitsAndYellowThings.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cartesian product\n",
    "print(fruits.cartesian(yellowThings).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.collect())\n",
    "print(fruits.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 'banana'), ('banana', 'bee'), ('canary melon', 'butter'), ('grap', 'canary melon'), ('lemon', 'gold'), ('orange', 'lemon'), ('pineapple', 'pineapple'), ('strawberry', 'sunflower')]\n",
      "[('apple', 0), ('banana', 1), ('canary melon', 2), ('grap', 3), ('lemon', 4), ('orange', 5), ('pineapple', 6), ('strawberry', 7)]\n",
      "[('apple', 0), ('banana', 2), ('canary melon', 4), ('grap', 6), ('lemon', 8), ('orange', 1), ('pineapple', 3), ('strawberry', 5)]\n"
     ]
    }
   ],
   "source": [
    "# zip \n",
    "#zip() function is used to combine values in both the RDD’s as pairs by returning a new RDD.\n",
    "#newyellowThings = yellowThings.repartition(3)\n",
    "print(fruits.zip(yellowThings).collect())\n",
    "\n",
    "#a new RDD of tuples containing positional index information\n",
    "print(fruits.zipWithIndex().collect())\n",
    "\n",
    "# Items in the kth partition will get ids k, n+k, 2*n+k, …, where n is the number of partitions. \n",
    "# This is more efficient since each partition is processed independently(useful when considering partitions)\n",
    "print(fruits.zipWithUniqueId().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect: fruitsArray(RDD: list like)\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "print(numFruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "print(first3Fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: Don't use count() when you don't need to return the exact number of rows\n",
    "# use take() or isEmpty()\n",
    "print(fruits.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits.map(lambda fruit: len(fruit)).sum())\n",
    "print(fruits.map(lambda fruit: len(fruit)).reduce(lambda x, y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reduce function must be associative,communicative; otherwise the result is nondeterministic \n",
    "#rdd = sc.parallelize([1, 2, 1, 3, 4, 5, 2], 4)\n",
    "rdd = sc.parallelize([1, 2, 3], 2)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.reduce(lambda x, y: 2*x+y))\n",
    "#reduce function is not associative, will perform on each partition first, then left to right partition-level op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treeReduce \n",
    "# Data are combined partially on a small set of executors before they are sent to the driver, \n",
    "# which dramatically reduces the load the driver has to deal with. \n",
    "\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).treeReduce(lambda x, y: x.union(y))\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letterSet = fruits.flatMap(lambda fruit: list(fruit)).distinct().collect()\n",
    "#list(\"shi jia\")->['s','h','i',\" \",\"j\",'i','a']\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold:Aggregate the elements of each partition, and then the results for \n",
    "# all the partitions, using a given associative function and a neutral “zero value.”\n",
    "# op(t1, t2) is allowed to modify t1(empty set here) and return it as its result value to avoid object allocation;\n",
    "# however, it should not modify t2.\n",
    "print(fruits.glom().collect())\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).fold(set(), lambda x, y: x.union(y))\n",
    "print(letterSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reducing an empty rdd is not allowed, but fold is OK.\n",
    "r = sc.parallelize([])\n",
    "#r.reduce(lambda x, y: x+y)\n",
    "r.fold(0, lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o', 'r', 'a', 'i', 'p', 'g', 'c', ' ', 'l', 'y', 'e', 'w', 'n', 'b', 'm', 't', 's'}\n"
     ]
    }
   ],
   "source": [
    "# aggregate / treeAggregate can return a different result type than the type of the RDD\n",
    "#aggregate: Aggregate the elements of each partition, and then the results for all the partitions, \n",
    "# using a given combine functions and a neutral “zero value.”\n",
    "#seqOp: an operator used to accumulate results within a partition\n",
    "#combOp:an associative operator used to combine results from different partitions\n",
    "def f(x, y):\n",
    "    # x is \"zero value\": empty set here, y is the elem is rdd\n",
    "    x.add(y)\n",
    "    return x\n",
    "\n",
    "letterSet = fruits.flatMap(lambda fruit: list(fruit)).treeAggregate(set(), f, lambda x, y: x.union(y))\n",
    "print(letterSet)\n",
    "\n",
    "# It avoids object allocation.\n",
    "# This is the most efficient way for solving this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I have a apple\n",
      "I have a banana\n",
      "I have a canary melon\n",
      "I have a grap\n",
      "I have a lemon\n",
      "I have a orange\n",
      "I have a pineapple\n",
      "I have a strawberry\n"
     ]
    }
   ],
   "source": [
    "# foreach is an action, map is a transformation.\n",
    "fruits.foreach(lambda x: print('I have a', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have a apple', 'I have a banana', 'I have a canary melon', 'I have a grap', 'I have a lemon', 'I have a orange', 'I have a pineapple', 'I have a strawberry']\n"
     ]
    }
   ],
   "source": [
    "print(fruits.map(lambda x: 'I have a ' + x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "0\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06 0\n",
      "1  1\n",
      "2 63\n",
      "\n",
      "7 13\n",
      "8 21\n",
      "9 30\n",
      "3 3\n",
      "4 7\n",
      "5 12\n"
     ]
    }
   ],
   "source": [
    "#closure is those variables and methods which must be visible for \n",
    "# the executor to perform its computations on the RDD.\n",
    "counter = 0# closure is just copy, values only change inside the function; need to be returned by the function(shared variable)\n",
    "rdd = sc.parallelize(range(10), 3)\n",
    "\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "    print(x, counter)\n",
    "\n",
    "print(rdd.collect())\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(counter)\n",
    "print(rdd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "#Worker tasks on a Spark cluster can add values to an Accumulator with the += operator, but only the \n",
    "#driver program is allowed to access its value, using value. Updates from the workers get propagated\n",
    "#automatically to the driver program.\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "\n",
    "a = rdd.foreach(g)\n",
    "\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "45\n",
      "45\n",
      "90\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    return x * x\n",
    "\n",
    "a = rdd.map(g)\n",
    "print(accum.value)#0 since map is a transformer, not action\n",
    "# print(a.reduce(lambda x, y: x+y))\n",
    "#a.cache()\n",
    "tmp = a.count()\n",
    "print(accum.value)#count is an action: accum's value is updated#45\n",
    "print(rdd.reduce(lambda x, y: x+y))#45\n",
    "\n",
    "tmp = a.count()#rdd.map(g) is computed twice by spark\n",
    "print(accum.value)#90\n",
    "print(rdd.reduce(lambda x, y: x+y))#45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:===================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999999950000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "rdd = sc.parallelize(range(1000000*n) , n)\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    return x\n",
    "\n",
    "a = rdd.map(g)\n",
    "tmp = a.count()\n",
    "print(accum.value)\n",
    "\n",
    "\n",
    "# correct answer: 4999999950000000\n",
    "# Spark will only update the accumulator from the successful task,and the failed tasks are completely ignored.\n",
    "# So the accumulator is computed correctly even if some tasks fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999999950000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "rdd1 = sc.parallelize(range(1000000*n) , n)\n",
    "rdd2 = sc.parallelize(range(1000000*n) , n)\n",
    "rdd = rdd1.zip(rdd2)\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x[0]\n",
    "    return x\n",
    "\n",
    "a = rdd.map(g)\n",
    "b = a.reduceByKey(sum)  # this causes a shuffle\n",
    "tmp = b.count()\n",
    "print(accum.value)\n",
    "\n",
    "# Because shuffle output is stored locally, if a node goes down, that shuffle output is gone. \n",
    "# Spark goes back to the stage that generated the shuffle output, looks at which tasks need\n",
    "# to be rerun, and re-executes them on one of the nodes that is still alive.\n",
    "# This results in the accumulator being an over-count\n",
    "\n",
    "# Summary: It's OK to use accumulators in an action (e.g., foreach) but not in a transformation\n",
    "# Or better: Avoid using them at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure and Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# Linear-time selection（quick select)\n",
    "#choose the k largest elem\n",
    "data = [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
    "A = sc.parallelize(data,2)\n",
    "k = 4\n",
    "\n",
    "while True:\n",
    "    x = A.first()\n",
    "    A1 = A.filter(lambda z: z < x)\n",
    "    A2 = A.filter(lambda z: z > x)\n",
    "    #A1.cache()#need to cache these; otherwise, A1's value is not changed(transformation,not action)\n",
    "    #A2.cache()\n",
    "    mid = A1.count()\n",
    "    if mid == k:\n",
    "        print(x)\n",
    "        break\n",
    "    if k < mid:\n",
    "        A = A1\n",
    "    else:\n",
    "        A = A2\n",
    "        k = k - mid - 1\n",
    "    A.cache()#option2: only cache A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#similar to the one in the quiz\n",
    "A = sc.parallelize(range(10)) \n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x)\n",
    "#B.cache() if cached, A is triggered once\n",
    "print(B.count())\n",
    "x = 3\n",
    "print(B.count())#Transformation is setting up a tunnel; action is flushing the data through the tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(10))\n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x)\n",
    "B.cache()  #the first time its is computed, the value is stored  (when caching, replace x with current value of 5)\n",
    "# In the Scala version of Spark, whether caching B or not will always return [0, 1, 2].\n",
    "# However, in PySpark, it seems that cache() actually resolves all variables in the transformation (filter in this example)\n",
    "# at the time the RDD is cached, but not at the time of an action that triggers the transformation.  \n",
    "# This inconsistency between the Python version and the Scala version of Spark has not been documented.  \n",
    "# If anyone can find more information, please share with us.\n",
    "print(B.take(10))\n",
    "#print(B.collect())\n",
    "\n",
    "x = 3\n",
    "print(B.take(10))\n",
    "print(B.collect())\n",
    "# B.collect() doesn't always re-collect data - bad design!\n",
    "# Always use take() instead of collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:================================================>      (88 + 8) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# From the official spark examples.\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 100\n",
    "n = 100000 * partitions\n",
    "#makes a circle of unit radius 1\n",
    "def f(_: int) -> float:\n",
    "    x = random() * 2 - 1 #random() returns [0,1]\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "#random seed generated in each partition is the same\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)# count/n = pi/4\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.744948514497711, 0.8954111195391162, 0.2218497023630468, 0.022007612366036433, 0.8910718215566176], [0.744948514497711, 0.8954111195391162, 0.2218497023630468, 0.022007612366036433, 0.8910718215566176]]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "#seed in the driver program won't be changed\n",
    "#all the computation is run on the worker(status won't changed)\n",
    "a = sc.parallelize(range(0,10),2)\n",
    "print(a.map(lambda _: random()).glom().collect())\n",
    "\n",
    "# Why are random numbers in all partitions the same? all the workers are generated the same number(\n",
    "#random number in spark generated same number every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8954111195391162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions and mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts']\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions: Return a new RDD by applying a function to each partition of this RDD.\n",
    "#map: 1 elem in,1 elem out| partition: 1 partition in and 1 partition out\n",
    "def f(pa):#pa is a partition list(iterator)\n",
    "    l = []\n",
    "    for x in pa:\n",
    "        l.append(x[::-1])\n",
    "    return l\n",
    "        \n",
    "fruitsReversed = fruits.mapPartitions(f)\n",
    "print(fruitsReversed.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elppa', 'ananab', 'nolem yranac', 'parg', 'nomel', 'egnaro', 'elppaenip', 'yrrebwarts']\n"
     ]
    }
   ],
   "source": [
    "# It's more efficient to use yield\n",
    "#pa entry point to a list\n",
    "def f(pa):\n",
    "    for x in pa:\n",
    "        yield x[::-1]#for 循环中的 yield 会把当前的元素记下来，保存在集合中，循环结束后将返回该集合\n",
    "        #for loop has a buffer you can't see, for loop finishes running, it will return \n",
    "        #this collection of all the yielded values from the unseen buffer.\n",
    "\n",
    "fruitsReversed = fruits.mapPartitions(f)\n",
    "print(fruitsReversed.collect())\n",
    "\n",
    "#  It provides a facility to do heavy initializations (for example Database connection) once for each partition\n",
    "# instead of doing it on every element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'banana', 'canary melon', 'grap', 'lemon'], ['orange', 'pineapple', 'strawberry']]\n",
      "['lemon', 'grap', 'canary melon', 'banana', 'apple', 'strawberry', 'pineapple', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# Can also do some transformation on the partition level\n",
    "\n",
    "def f(pa):\n",
    "    return sorted(pa, reverse = True)\n",
    "\n",
    "print(fruits.glom().collect())\n",
    "print(fruits.mapPartitions(f).collect())#sort on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0apple', '0banana', '0canary melon', '0grap', '0lemon', '1orange', '1pineapple', '1strawberry']\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "#i is the partition index\n",
    "def f(i, pa):# do partition specific things\n",
    "    for x in pa:\n",
    "        yield str(i) + x\n",
    "\n",
    "fruitsIndexd = fruits.mapPartitionsWithIndex(f)\n",
    "print(fruitsIndexd.collect())\n",
    "\n",
    "# These two functions will be useful for advanced algorithm design (will see later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:===================================================> (970 + 8) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.14170008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:====================================================>(986 + 8) / 1000]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Correct version for computing Pi\n",
    "from random import random, seed\n",
    "from time import time\n",
    "\n",
    "partitions = 1000\n",
    "n = 100000 * partitions\n",
    "\n",
    "s = time()\n",
    "\n",
    "def f(index, it):\n",
    "    seed(index + s)#the seed in each partiton is different, can generate different random number\n",
    "    for i in it:\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        yield 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions).mapPartitionsWithIndex(f).sum()\n",
    "\n",
    "print(\"Pi is roughly\", 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1103785573901771, 0.8930514571020657, 0.9095138951659408, 0.48617438764010323, 0.7785035676844477], [0.178013983301885, 0.8953354693764833, 0.2642794179289347, 0.14804273449930028, 0.1521417802028162]]\n"
     ]
    }
   ],
   "source": [
    "# Check that random numbers are different\n",
    "from random import random, seed\n",
    "from time import time\n",
    "\n",
    "s = time()\n",
    "\n",
    "def f(index, it):\n",
    "    seed(index + s)\n",
    "    for i in it:\n",
    "        yield random()\n",
    "\n",
    "print(sc.parallelize(range(10), 2).mapPartitionsWithIndex(f).glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, <pyspark.resultiterable.ResultIterable object at 0x7fb755b6a6a0>), (12, <pyspark.resultiterable.ResultIterable object at 0x7fb755b47310>), (4, <pyspark.resultiterable.ResultIterable object at 0x7fb755b47d60>), (10, <pyspark.resultiterable.ResultIterable object at 0x7fb755ca69d0>), (5, <pyspark.resultiterable.ResultIterable object at 0x7fb755ca6370>), (9, <pyspark.resultiterable.ResultIterable object at 0x7fb755ca62b0>)]\n",
      "banana\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "# groupByKey only on tuples with (a,b)->(key,value) pair\n",
    "groupFruitsByLength = fruits.map(lambda fruit: (len(fruit), fruit)).groupByKey()\n",
    "print(groupFruitsByLength.take(10))\n",
    "for x in groupFruitsByLength.take(1)[0][1]:\n",
    "    print(x)#return the fruit with length 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 2), (12, 1), (4, 1), (10, 1), (5, 2), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# count the number of fruits by length: mimics MapReduce\n",
    "#mapValues: map function for value in group by key(values with the same key)\n",
    "print(fruits.map(lambda fruit: (len(fruit), 1)).groupByKey().mapValues(sum).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 2), (12, 1), (4, 1), (10, 1), (5, 2), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey: this more efficient\n",
    "# reduceByKey will compute local sums for each key in each partition and combine those local sums \n",
    "# into larger sums after shuffling.(do some local shuffling first)\n",
    "\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "print(numFruitsByLength.take(10))\n",
    "\n",
    "# aggregateByKey, foldByKey also available.\n",
    "# but there is no treeAggregateByKey(在每个RDD里的，跟driver没关系）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 3), ('data', 4), ('and', 3)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "lines = sc.textFile('course.txt')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add).filter(lambda x: x[1]>=3)\n",
    "print(counts.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#collectAsMap: turn list into dictio\n",
    "print(counts.collectAsMap())#collectAsMap()['data'] find the count of specific key value pairds; not efficient\n",
    "#(return the whole rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data', 4)\n"
     ]
    }
   ],
   "source": [
    "print(counts.sortBy(lambda x: x[1], False).take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.lookup('data')\n",
    "# This scans the whole RDD, unless there is a partitioner (to be discussed later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join vs. Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join simple example\n",
    "\n",
    "products = sc.parallelize([(1, \"Apple\"),(1,\"apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "#trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print(products.join(trans).take(20))#join on key value(1,2,3...)-> nested struc(key,value from 1st RDD, value from 2nd RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = {1: \"Apple\", 2: \"Orange\", 3: \"TV\", 4: \"PC\", 5: \"Computer\"}\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "broadcasted_products = sc.broadcast(products)# broadcasted_products.value[x[0](key)]==\"Apple\"\n",
    "# broadcast all the servers can get the shared broadcasted variables \n",
    "results = trans.map(lambda x: (x[0], broadcasted_products.value[x[0]], x[1]))#structure of the result\n",
    "#  results = trans.map(lambda x: (x[0], products[x[0]], x[1]))\n",
    "print(results.take(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with cogroup(more efficient)\n",
    "\n",
    "products = sc.parallelize([(1, \"Apple\"),(1,\"apple\"), (2, \"Orange\"), (3, \"TV\"), (4, \"PC\"), (5, \"Computer\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "for x,y in products.cogroup(trans).collect():\n",
    "    print(x, tuple(map(list, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "aggregateByKey() missing 2 required positional arguments: 'seqFunc' and 'combFunc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/ipykernel_822/1202785205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#print(\"nothing\",closest.collect())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpointStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregateByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# sum of pointer and sum of counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# pointStats is an RDD of tuples (index of center(same key,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# (array of sums of coordinates(np:+entry-level add), total number of points assigned))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: aggregateByKey() missing 2 required positional arguments: 'seqFunc' and 'combFunc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split()])\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt\n",
    "lines = sc.textFile('kmeans_data.txt', 5)  \n",
    "\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt\n",
    "# lines = sc.textFile('../data/kmeans_bigdata.txt', 5)  \n",
    "# lines is an RDD of strings\n",
    "#print(lines.collect())\n",
    "K = 3\n",
    "convergeDist = 0.01  #（floating point can never check equality-> precision problem check error range can be done）\n",
    "# terminate algorithm when the total distance from old center to new centers is less than this value\n",
    "\n",
    "data = lines.map(parseVector).cache() # data is an RDD of arrays must cache()\n",
    "\n",
    "#takeSample(withReplacement: bool, num: int, seed: Optional[int] = None) \n",
    "#Return a fixed-size sampled subset of this RDD.\n",
    "kCenters = data.takeSample(False, K, 1)  # intial centers as a list of arrays 1:seed for sampling\n",
    "tempDist = 1.0  # total distance from old centers to new centers\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(lambda p: (closestPoint(p, kCenters), (p, 1)))\n",
    "    # for each point in data, find its closest center\n",
    "    # closest is an RDD of tuples (index of closest center, (point, 1))\n",
    "    #print(\"nothing\",closest.collect())\n",
    "        \n",
    "    #pointStats = closest.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))# sum of pointer and sum of counter\n",
    "    \n",
    "    # pointStats is an RDD of tuples (index of center(same key,\n",
    "    # (array of sums of coordinates(np:+entry-level add), total number of points assigned))\n",
    "    # aggregate by Key is more efficient\n",
    "    #print(\"test\",pointStats.collect())\n",
    "    \n",
    "    newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "    # compute the new centers\n",
    "    \n",
    "    tempDist = sum(np.sum((kCenters[i] - p) ** 2) for (i, p) in newCenters)\n",
    "    # compute the total disctance from old centers to new centers\n",
    "    \n",
    "    for (i, p) in newCenters:\n",
    "        kCenters[i] = p\n",
    "        \n",
    "print(\"Final centers: \", kCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', <pyspark.resultiterable.ResultIterable at 0x7fe253cbf250>),\n",
       " ('4', <pyspark.resultiterable.ResultIterable at 0x7fe253cbffa0>),\n",
       " ('2', <pyspark.resultiterable.ResultIterable at 0x7fe253ce1400>),\n",
       " ('3', <pyspark.resultiterable.ResultIterable at 0x7fe253ce1430>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "lines = sc.textFile(\"pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"../data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "links.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1.2981882732854677), ('4', 0.9999999999999998), ('3', 0.9999999999999998), ('2', 0.7018117267145316)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    #each destination in urls equally share the page rank\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)#yield is used in for-loop and writes a new element into the resulting sequence.\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL(source)         neighbor URL(destination)\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "lines = sc.textFile(\"pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"../data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0)#do mapping only for value in the list of (key,value) pairs\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks)  \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls, rank))\n",
    "    # join: Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "    \n",
    "    #contribs is of form:[(u1,1/3),(u2,1/3),...,...] key: destincation url, value: contribution to this destination\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    # ranks = contribs.reduceByKey(add).map(lambda t: (t[0], t[1] * 0.85 + 0.15))\n",
    "\n",
    "print(ranks.top(5, lambda x: x[1]))#only one action ; for loop only construct the lineage graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
