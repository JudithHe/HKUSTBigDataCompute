{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Denny', 31.0), ('Jules', 30.0), ('Brooke', 22.5), ('TD', 35.0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)])\n",
    "\n",
    "# Use map and reduceByKey transformations with their lambda\n",
    "# expressions to aggregate and then compute average\n",
    "agesRDD = (dataRDD.map(lambda x: (x[0], (x[1], 1))) \n",
    ".reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    ".map(lambda x: (x[0], x[1][0]/x[1][1])))\n",
    "\n",
    "agesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                       (0 + 48) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "    (\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "StructType([StructField('Id', StringType(), True), StructField('First', StringType(), True), StructField('Last', StringType(), True), StructField('Url', StringType(), True), StructField('Published', StringType(), True), StructField('Hits', IntegerType(), True), StructField('Campaigns', ArrayType(StringType(), True), True)])\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for our data using DDL\n",
    "\n",
    "schema = \"Id: STRING, First: STRING, Last: STRING, Url: STRING, Published: STRING, Hits: INT, Campaigns: ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "            [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\"LinkedIn\"]],\n",
    "            [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "            [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,[\"twitter\", \"FB\"]],\n",
    "            [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "            [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,[\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.schema)\n",
    "blogs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| _1|       _2|     _3|               _4|       _5|   _6|                  _7|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame without specifying schema: the schema will be inferred\n",
    "\n",
    "blogs_df2 = spark.createDataFrame(data)\n",
    "blogs_df2.show()\n",
    "blogs_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: long (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "blogs_df2 = spark.createDataFrame(data, ['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns'])\n",
    "blogs_df2.show()\n",
    "blogs_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='1', First='Jules', Last='Damji', Url='https://tinyurl.1', Published='1/4/2016', Hits=4535, Campaigns=['twitter', 'LinkedIn']),\n",
       " Row(Id='2', First='Brooke', Last='Wenig', Url='https://tinyurl.2', Published='5/5/2018', Hits=8908, Campaigns=['twitter', 'LinkedIn'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "blogs_df.rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jules'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.rdd.take(1)[0]['First']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.rdd.take(1)[0].Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.rdd.take(1)[0].Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Row.count(value, /)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try changing the column name to 'count'.   Then this will not work due to naming conflict \n",
    "blogs_df.rdd.take(1)[0].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Id'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access a particular column with col and it returns a Column type\n",
    "blogs_df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Id'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to access a column - but be aware of naming conflicts\n",
    "blogs_df.Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|    First|\n",
      "+---+---------+\n",
      "|  1|    Jules|\n",
      "|  2|   Brooke|\n",
      "|  3|    Denny|\n",
      "|  4|Tathagata|\n",
      "|  5|    Matei|\n",
      "|  6|  Reynold|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection: Retrieve specific columns from the dataframe\n",
    "blogs_df.select('Id', 'First').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----------------+---------+----+--------------------+\n",
      "| Id| First| Last|              Url|Published|Hits|           Campaigns|\n",
      "+---+------+-----+-----------------+---------+----+--------------------+\n",
      "|  2|Brooke|Wenig|https://tinyurl.2| 5/5/2018|8908| [twitter, LinkedIn]|\n",
      "|  3| Denny|  Lee|https://tinyurl.3| 6/7/2019|7659|[web, twitter, FB...|\n",
      "+---+------+-----+-----------------+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Selection: Retrieve specific rows\n",
    "blogs_df.filter(\"First<'G'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID|(Hits + 1)|\n",
      "+---+----------+\n",
      "|  1|      4536|\n",
      "|  2|      8909|\n",
      "|  3|      7660|\n",
      "|  4|     10569|\n",
      "|  5|     40579|\n",
      "|  6|     25569|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a column (equivalent to map)\n",
    "\n",
    "blogs_df.select('ID', blogs_df.Hits + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| Id|(Hits + 1)|\n",
      "+---+----------+\n",
      "|  1|      4536|\n",
      "|  2|      8909|\n",
      "|  3|      7660|\n",
      "|  4|     10569|\n",
      "|  5|     40579|\n",
      "|  6|     25569|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way:\n",
    "\n",
    "blogs_df.select('Id', col('Hits') + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|NewHits|\n",
      "+---+-------+\n",
      "|  1|   4536|\n",
      "|  2|   8909|\n",
      "|  3|   7660|\n",
      "|  4|  10569|\n",
      "|  5|  40579|\n",
      "|  6|  25569|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# giving a name to the new column:\n",
    "\n",
    "blogs_df.select('Id', blogs_df.Hits + 1).withColumnRenamed('(Hits + 1)', 'NewHits').show()\n",
    "\n",
    "# Because DataFrame transformations are immutable, when we\n",
    "# rename a column using withColumnRenamed() we get a new Data‐\n",
    "# Frame while retaining the original with the old column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| Id|NewHits|\n",
      "+---+-------+\n",
      "|  1|   4536|\n",
      "|  2|   8909|\n",
      "|  3|   7660|\n",
      "|  4|  10569|\n",
      "|  5|  40579|\n",
      "|  6|  25569|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# giving a name to the new column:\n",
    "\n",
    "blogs_df.select('Id', (blogs_df.Hits + 1).alias('NewHits')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column\n",
    "\n",
    "blogs_df.sort('Hits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column in descending order\n",
    "\n",
    "blogs_df.sort('Hits', ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column in descending order\n",
    "\n",
    "blogs_df.sort(col('Hits').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load an external file as a DataFrame\n",
    "# Download data at https://www.cse.ust.hk/msbd5003/data/sf-fire-calls.csv \n",
    "\n",
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "    StructField('UnitID', StringType(), True),\n",
    "    StructField('IncidentNumber', IntegerType(), True),\n",
    "    StructField('CallType', StringType(), True),\n",
    "    StructField('CallDate', StringType(), True),\n",
    "    StructField('WatchDate', StringType(), True),\n",
    "    StructField('CallFinalDisposition', StringType(), True),\n",
    "    StructField('AvailableDtTm', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('Zipcode', IntegerType(), True),\n",
    "    StructField('Battalion', StringType(), True),\n",
    "    StructField('StationArea', StringType(), True),\n",
    "    StructField('Box', StringType(), True),\n",
    "    StructField('OriginalPriority', StringType(), True),\n",
    "    StructField('Priority', StringType(), True),\n",
    "    StructField('FinalPriority', IntegerType(), True),\n",
    "    StructField('ALSUnit', BooleanType(), True),\n",
    "    StructField('CallTypeGroup', StringType(), True),\n",
    "    StructField('NumAlarms', IntegerType(), True),\n",
    "    StructField('UnitType', StringType(), True),\n",
    "    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "    StructField('FirePreventionDistrict', StringType(), True),\n",
    "    StructField('SupervisorDistrict', StringType(), True),\n",
    "    StructField('Neighborhood', StringType(), True),\n",
    "    StructField('Location', StringType(), True),\n",
    "    StructField('RowID', StringType(), True),\n",
    "    StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"../data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n",
    "\n",
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection\n",
    "few_fire_df = fire_df.select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \\\n",
    "    .where(col(\"CallType\") != \"Medical Incident\")\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        CallType|\n",
      "+----------------+\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|    Vehicle Fire|\n",
      "|          Alarms|\n",
      "|  Structure Fire|\n",
      "|          Alarms|\n",
      "|          Alarms|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|  Structure Fire|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "|Medical Incident|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Projection doesn't remove duplicates:\n",
    "\n",
    "fire_df.select('CallType').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            CallType|\n",
      "+--------------------+\n",
      "|Elevator / Escala...|\n",
      "|  Aircraft Emergency|\n",
      "|              Alarms|\n",
      "|Odor (Strange / U...|\n",
      "|Citizen Assist / ...|\n",
      "|              HazMat|\n",
      "|           Explosion|\n",
      "|           Oil Spill|\n",
      "|        Vehicle Fire|\n",
      "|  Suspicious Package|\n",
      "|Extrication / Ent...|\n",
      "|               Other|\n",
      "|        Outside Fire|\n",
      "|   Traffic Collision|\n",
      "|       Assist Police|\n",
      "|Gas Leak (Natural...|\n",
      "|        Water Rescue|\n",
      "|   Electrical Hazard|\n",
      "|   High Angle Rescue|\n",
      "|      Structure Fire|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 167:=====>                                                 (1 + 10) / 11]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use distinct to remove duplicates:\n",
    "fire_df.select('CallType').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct count:\n",
    "fire_df.select('CallType').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is more efficient:\n",
    "fire_df.select('CallType').agg(count_distinct('CallType').alias(\"DistinctCallTypes\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Like SQL, SparkSQL supports many type-specific operations\n",
    "\n",
    "fire_ts_df = (fire_df\n",
    "    .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"CallDate\")\n",
    "    .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    "    .drop(\"WatchDate\")\n",
    "    .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    "    \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "    .drop(\"AvailableDtTm\"))\n",
    "\n",
    "# Select the converted columns\n",
    "fire_ts_df.select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "    .select(year('IncidentDate'))\n",
    "    .distinct()\n",
    "    .orderBy(year('IncidentDate'))\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What were the most common types of fire calls?\n",
    "\n",
    "(fire_ts_df\n",
    "    .select(\"CallType\")\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .groupBy(\"CallType\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-----------+----------+\n",
      "|sum(NumAlarms)|       avg(Delay)| min(Delay)|max(Delay)|\n",
      "+--------------+-----------------+-----------+----------+\n",
      "|        176170|3.892364154521585|0.016666668|   1844.55|\n",
      "+--------------+-----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More aggregation functions\n",
    "\n",
    "(fire_ts_df\n",
    "    .select(sum(\"NumAlarms\"), avg(\"Delay\"),\n",
    "    min(\"Delay\"), max(\"Delay\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------+\n",
      "|CallType                     |avg(Delay)        |\n",
      "+-----------------------------+------------------+\n",
      "|Elevator / Escalator Rescue  |4.337821918278603 |\n",
      "|Aircraft Emergency           |3.773148145940569 |\n",
      "|Alarms                       |3.5427290570836676|\n",
      "|Odor (Strange / Unknown)     |4.9479591785645   |\n",
      "|Citizen Assist / Service Call|5.47334257750617  |\n",
      "|HazMat                       |7.527016133069992 |\n",
      "|Explosion                    |4.110674162259262 |\n",
      "|Oil Spill                    |4.977777770587376 |\n",
      "|Vehicle Fire                 |3.903922712007228 |\n",
      "|Suspicious Package           |6.576666704813639 |\n",
      "+-----------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    "    .where(col(\"CallType\").isNotNull())\n",
    "    .groupBy(\"CallType\")\n",
    "    .avg(\"Delay\")\n",
    "    .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "\n",
    "dfCustomer = spark.read.csv('../data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('../data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('../data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('../data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+---------+\n",
      "|ProductID|Name                         |ListPrice|\n",
      "+---------+-----------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58    |1431.5   |\n",
      "|708      |Sport-100 Helmet, Black      |34.99    |\n",
      "|722      |LL Road Frame - Black, 58    |337.22   |\n",
      "|723      |LL Road Frame - Black, 60    |337.22   |\n",
      "|724      |LL Road Frame - Black, 62    |337.22   |\n",
      "|736      |LL Road Frame - Black, 44    |337.22   |\n",
      "|737      |LL Road Frame - Black, 48    |337.22   |\n",
      "|738      |LL Road Frame - Black, 52    |337.22   |\n",
      "|743      |HL Mountain Frame - Black, 42|1349.6   |\n",
      "|744      |HL Mountain Frame - Black, 44|1349.6   |\n",
      "|745      |HL Mountain Frame - Black, 48|1349.6   |\n",
      "|746      |HL Mountain Frame - Black, 46|1349.6   |\n",
      "|747      |HL Mountain Frame - Black, 38|1349.6   |\n",
      "|765      |Road-650 Black, 58           |782.99   |\n",
      "|766      |Road-650 Black, 60           |782.99   |\n",
      "|767      |Road-650 Black, 62           |782.99   |\n",
      "|768      |Road-650 Black, 44           |782.99   |\n",
      "|769      |Road-650 Black, 48           |782.99   |\n",
      "|770      |Road-650 Black, 52           |782.99   |\n",
      "|775      |Mountain-100 Black, 38       |3374.99  |\n",
      "+---------+-----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+------------+\n",
      "|ProductID|Name                         |Double price|\n",
      "+---------+-----------------------------+------------+\n",
      "|680      |HL Road Frame - Black, 58    |2863.0      |\n",
      "|708      |Sport-100 Helmet, Black      |69.98       |\n",
      "|722      |LL Road Frame - Black, 58    |674.44      |\n",
      "|723      |LL Road Frame - Black, 60    |674.44      |\n",
      "|724      |LL Road Frame - Black, 62    |674.44      |\n",
      "|736      |LL Road Frame - Black, 44    |674.44      |\n",
      "|737      |LL Road Frame - Black, 48    |674.44      |\n",
      "|738      |LL Road Frame - Black, 52    |674.44      |\n",
      "|743      |HL Mountain Frame - Black, 42|2699.2      |\n",
      "|744      |HL Mountain Frame - Black, 44|2699.2      |\n",
      "|745      |HL Mountain Frame - Black, 48|2699.2      |\n",
      "|746      |HL Mountain Frame - Black, 46|2699.2      |\n",
      "|747      |HL Mountain Frame - Black, 38|2699.2      |\n",
      "|765      |Road-650 Black, 58           |1565.98     |\n",
      "|766      |Road-650 Black, 60           |1565.98     |\n",
      "|767      |Road-650 Black, 62           |1565.98     |\n",
      "|768      |Road-650 Black, 44           |1565.98     |\n",
      "|769      |Road-650 Black, 48           |1565.98     |\n",
      "|770      |Road-650 Black, 52           |1565.98     |\n",
      "|775      |Mountain-100 Black, 38       |6749.98     |\n",
      "+---------+-----------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct['ListPrice'] * 2).alias('Double price')) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "|718      |HL Road Frame - Red, 44  |2863.0         |\n",
      "|719      |HL Road Frame - Red, 48  |2863.0         |\n",
      "|720      |HL Road Frame - Red, 52  |2863.0         |\n",
      "|721      |HL Road Frame - Red, 56  |2863.0         |\n",
      "|722      |LL Road Frame - Black, 58|674.44         |\n",
      "|723      |LL Road Frame - Black, 60|674.44         |\n",
      "|724      |LL Road Frame - Black, 62|674.44         |\n",
      "|725      |LL Road Frame - Red, 44  |674.44         |\n",
      "|726      |LL Road Frame - Red, 48  |674.44         |\n",
      "|727      |LL Road Frame - Red, 52  |674.44         |\n",
      "|728      |LL Road Frame - Red, 58  |674.44         |\n",
      "|729      |LL Road Frame - Red, 60  |674.44         |\n",
      "|730      |LL Road Frame - Red, 62  |674.44         |\n",
      "|731      |ML Road Frame - Red, 44  |1189.66        |\n",
      "|732      |ML Road Frame - Red, 48  |1189.66        |\n",
      "|733      |ML Road Frame - Red, 52  |1189.66        |\n",
      "|734      |ML Road Frame - Red, 58  |1189.66        |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+---------+\n",
      "|ProductID|Name                      |ListPrice|\n",
      "+---------+--------------------------+---------+\n",
      "|860      |Half-Finger Gloves, L     |24.49    |\n",
      "|859      |Half-Finger Gloves, M     |24.49    |\n",
      "|858      |Half-Finger Gloves, S     |24.49    |\n",
      "|708      |Sport-100 Helmet, Black   |34.99    |\n",
      "|862      |Full-Finger Gloves, M     |37.99    |\n",
      "|861      |Full-Finger Gloves, S     |37.99    |\n",
      "|863      |Full-Finger Gloves, L     |37.99    |\n",
      "|841      |Men's Sports Shorts, S    |59.99    |\n",
      "|849      |Men's Sports Shorts, M    |59.99    |\n",
      "|851      |Men's Sports Shorts, XL   |59.99    |\n",
      "|850      |Men's Sports Shorts, L    |59.99    |\n",
      "|815      |LL Mountain Front Wheel   |60.745   |\n",
      "|868      |Women's Mountain Shorts, M|69.99    |\n",
      "|869      |Women's Mountain Shorts, L|69.99    |\n",
      "|867      |Women's Mountain Shorts, S|69.99    |\n",
      "|853      |Women's Tights, M         |74.99    |\n",
      "|854      |Women's Tights, L         |74.99    |\n",
      "|852      |Women's Tights, S         |74.99    |\n",
      "|818      |LL Road Front Wheel       |85.565   |\n",
      "|823      |LL Mountain Rear Wheel    |87.745   |\n",
      "+---------+--------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\") \\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [SalesOrderID#572, SalesOrderDetailID#573, Name#523, UnitPrice#576, OrderQty#574]\n",
      "+- *(2) BroadcastHashJoin [ProductID#575], [ProductID#522], Inner, BuildLeft\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#609]\n",
      "   :  +- *(1) Project [SalesOrderID#572, SalesOrderDetailID#573, OrderQty#574, ProductID#575, UnitPrice#576]\n",
      "   :     +- *(1) Filter isnotnull(ProductID#575)\n",
      "   :        +- FileScan csv [SalesOrderID#572,SalesOrderDetailID#573,OrderQty#574,ProductID#575,UnitPrice#576] Batched: false, DataFilters: [isnotnull(ProductID#575)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- *(2) Project [ProductID#522, Name#523]\n",
      "      +- *(2) Filter ((isnotnull(Color#525) AND (Color#525 = Black)) AND isnotnull(ProductID#522))\n",
      "         +- FileScan csv [ProductID#522,Name#523,Color#525] Batched: false, DataFilters: [isnotnull(Color#525), (Color#525 = Black), isnotnull(ProductID#522)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.show()\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL performs optimization depending on whether intermediate dataframe are cached or not:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.persist()\n",
    "d1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   29.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [SalesOrderID#572, SalesOrderDetailID#573, Name#523, UnitPrice#576, OrderQty#574]\n",
      "+- *(2) BroadcastHashJoin [ProductID#575], [ProductID#522], Inner, BuildLeft\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint))), [id=#1318]\n",
      "   :  +- *(1) Project [SalesOrderID#572, SalesOrderDetailID#573, OrderQty#574, ProductID#575, UnitPrice#576]\n",
      "   :     +- *(1) Filter isnotnull(ProductID#575)\n",
      "   :        +- FileScan csv [SalesOrderID#572,SalesOrderDetailID#573,OrderQty#574,ProductID#575,UnitPrice#576] Batched: false, DataFilters: [isnotnull(ProductID#575)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- *(2) Project [ProductID#522, Name#523]\n",
      "      +- *(2) Filter ((isnotnull(Color#525) AND (Color#525 = Black)) AND isnotnull(ProductID#522))\n",
      "         +- FileScan csv [ProductID#522,Name#523,Color#525] Batched: false, DataFilters: [isnotnull(Color#525), (Color#525 = Black), isnotnull(ProductID#522)], Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2 = d1.where(\"Color = 'Black'\")\n",
    "#d2 = d1.where(\"OrderQty >= 10\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation[SalesOrderID#1075,SalesOrderDetailID#1076,Name#1077,UnitPrice#1078,OrderQty#1079] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-168595bb0376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Color = 'Black'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \"\"\"\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`Color`' given input columns: [Name, OrderQty, SalesOrderDetailID, SalesOrderID, UnitPrice]; line 1 pos 0;\n'Filter ('Color = Black)\n+- Relation[SalesOrderID#1075,SalesOrderDetailID#1076,Name#1077,UnitPrice#1078,OrderQty#1079] csv\n"
     ]
    }
   ],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"Color\" among (SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty);",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-cbb1ee4162a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfDetail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfProduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ProductID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SalesOrderID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalesOrderDetailID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UnitPrice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OrderQty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Because the parser will try to find a column named 'Color' in d1, which doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \"\"\"\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/csproject/msbd5003/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"Color\" among (SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty);"
     ]
    }
   ],
   "source": [
    "# This will report an error, too\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(d1['Color'] == 'Black').show()\n",
    "\n",
    "# Because the parser will try to find a column named 'Color' in d1, which doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|     sum(netprice)|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74205.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|26677.883999999995|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33779.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936|      44490.290424|\n",
      "|       71780|         16964.322|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct, 'ProductID') \\\n",
    "        .where(\"Color = 'Black'\")\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|     29654|   Yao-Qiang|       Cheng|         null|\n",
      "|       472|         Ann|       Evans|         null|\n",
      "|     29974|       Scott|   MacDonald|         null|\n",
      "+----------+------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: https://www.cse.ust.hk/msbd5003/data/departuredelays.csv\n",
    "\n",
    "csv_file = \"../data/departuredelays.csv\"\n",
    "# Read and create a temporary view\n",
    "df = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "    FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "    ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM us_delay_flights_tbl\n",
    "    WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD'\n",
    "    ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 217:=======>                                                 (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "    CASE\n",
    "        WHEN delay > 360 THEN 'Very Long Delays'\n",
    "        WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "        WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "        WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "        WHEN delay = 0 THEN 'No Delays'\n",
    "        ELSE 'Early'\n",
    "    END AS Flight_Delays\n",
    "    FROM us_delay_flights_tbl\n",
    "    ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|3061605| 1004|   HNL|        LAX|\n",
      "|1311800| 1004|   HNL|        DFW|\n",
      "|2121625|  932|   HNL|        JFK|\n",
      "|2102155|  724|   HNL|        SFO|\n",
      "|2171300|  626|   HNL|        LAX|\n",
      "|2040725|  622|   HNL|        SFO|\n",
      "|3020745|  600|   HNL|        LAX|\n",
      "|2141405|  586|   HNL|        SFO|\n",
      "|2271255|  521|   HNL|        PHX|\n",
      "|2281300|  474|   HNL|        LAX|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('origin = \"HNL\"').createOrReplaceTempView('HNLflights')\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "    FROM HNLflights\n",
    "    WHERE delay > 120 \n",
    "    ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------+\n",
      "|   date|delay|distance|origin|destination|newdelay|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "|1011245|    6|     602|   ABE|        ATL|      13|\n",
      "|1020600|   -8|     369|   ABE|        DTW|     -15|\n",
      "|1021245|   -2|     602|   ABE|        ATL|      -3|\n",
      "|1020605|   -4|     602|   ABE|        ATL|      -7|\n",
      "|1031245|   -4|     602|   ABE|        ATL|      -7|\n",
      "|1030605|    0|     602|   ABE|        ATL|       1|\n",
      "|1041243|   10|     602|   ABE|        ATL|      21|\n",
      "|1040605|   28|     602|   ABE|        ATL|      57|\n",
      "|1051245|   88|     602|   ABE|        ATL|     177|\n",
      "|1050605|    9|     602|   ABE|        ATL|      19|\n",
      "|1061215|   -6|     602|   ABE|        ATL|     -11|\n",
      "|1061725|   69|     602|   ABE|        ATL|     139|\n",
      "|1061230|    0|     369|   ABE|        DTW|       1|\n",
      "|1060625|   -3|     602|   ABE|        ATL|      -5|\n",
      "|1070600|    0|     369|   ABE|        DTW|       1|\n",
      "|1071725|    0|     602|   ABE|        ATL|       1|\n",
      "|1071230|    0|     369|   ABE|        DTW|       1|\n",
      "|1070625|    0|     602|   ABE|        ATL|       1|\n",
      "|1071219|    0|     569|   ABE|        ORD|       1|\n",
      "|1080600|    0|     369|   ABE|        DTW|       1|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "myf = udf(lambda s: s*2 + 1, IntegerType())\n",
    "df.select('*', myf(df['delay']).alias('newdelay')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+--------+\n",
      "|   date|delay|distance|origin|destination|newdelay|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "|1011800|   39|    3288|   HNL|        DFW|      79|\n",
      "|1011700|   -1|    3687|   HNL|        ORD|      -1|\n",
      "|1011950|   -3|    3288|   HNL|        DFW|      -5|\n",
      "|1011340|   -8|    2221|   HNL|        LAX|     -15|\n",
      "|1010830|  -13|    2221|   HNL|        LAX|     -25|\n",
      "|1011615|  -16|    2221|   HNL|        LAX|     -31|\n",
      "|1012155|    7|    2221|   HNL|        LAX|      15|\n",
      "|1021800|  319|    3288|   HNL|        DFW|     639|\n",
      "|1021700|    0|    3687|   HNL|        ORD|       1|\n",
      "|1021950|    2|    3288|   HNL|        DFW|       5|\n",
      "|1021340|    3|    2221|   HNL|        LAX|       7|\n",
      "|1020830|  -14|    2221|   HNL|        LAX|     -27|\n",
      "|1021615|   19|    2221|   HNL|        LAX|      39|\n",
      "|1022155|  130|    2221|   HNL|        LAX|     261|\n",
      "|1031800|   54|    3288|   HNL|        DFW|     109|\n",
      "|1031700|   11|    3687|   HNL|        ORD|      23|\n",
      "|1031950|  239|    3288|   HNL|        DFW|     479|\n",
      "|1031340|   13|    2221|   HNL|        LAX|      27|\n",
      "|1030830|  -10|    2221|   HNL|        LAX|     -19|\n",
      "|1031615|    7|    2221|   HNL|        LAX|      15|\n",
      "+-------+-----+--------+------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('myf', lambda s: s*2+1, IntegerType())\n",
    "spark.sql('SELECT *, myf(Delay) AS newdelay FROM HNLflights').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Parquet files\n",
    "Parquet is an open source columnar file format that offers many I/O\n",
    "optimizations (such as compression, which saves storage space and allows for quick\n",
    "access to data columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data at https://www.cse.ust.hk/msbd5003/data/sf-airbnb.rar\n",
    "\n",
    "filePath = '../data/sf-airbnb-clean.parquet/'\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "airbnbDF.printSchema()\n",
    "\n",
    "# Our goal is to predict the price per night for a rental property, given our features.\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|      dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5, 7.0, 12.0]|  2|An ice sculpture| 12.5|[cold, ice]|   [-78.75, 20.4]|\n",
      "| [1.0, 3.1, 1.0]|  3|    A blue mouse| 25.5|       null|    [54.4, -32.7]|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dimensions.height|\n",
      "+-----------------+\n",
      "|              9.5|\n",
      "|              1.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)),\n",
       " Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|array_remove(tags, ice)|\n",
      "+-----------------------+\n",
      "|                 [cold]|\n",
      "|                   null|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Functions for arrays\n",
    "df.select(array_remove(df.tags, 'ice')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(tags)|\n",
      "+----------+\n",
      "|         2|\n",
      "|        -1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(size(df.tags)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|big tags|\n",
      "+--------+\n",
      "|   [ice]|\n",
      "|    null|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(filter(df.tags, lambda x: x > 'd').alias('big tags')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_1#25L) AND (_1#25L < 5))\n",
      "+- *(1) Scan ExistingRDD[_1#25L,_2#26L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(((cast((_1#2416L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#2497]\n",
      "+- *(1) Scan ExistingRDD[_1#2416L,_2#2417L]\n",
      "\n",
      "\n",
      "+----------------------------+\n",
      "|((((_1 * 2) + 2.5) + 1) + 1)|\n",
      "+----------------------------+\n",
      "|                         4.5|\n",
      "|                         6.5|\n",
      "|                         8.5|\n",
      "|                        10.5|\n",
      "|                        12.5|\n",
      "|                        14.5|\n",
      "|                        16.5|\n",
      "|                        18.5|\n",
      "|                        20.5|\n",
      "|                        22.5|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df1.explain()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print(a.take(10))\n",
    "x = 3\n",
    "print(a.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
