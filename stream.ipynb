{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPEC1CSbb244",
    "outputId": "4b97f897-dd06-4e78-e013-c2873629ddba",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a StreamingContext(sc stands for SparkContext) with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)#5 is btach size\n",
    "\n",
    "# Create a DStream that will connect to localhost at port 9999\n",
    "# Start Netcat server: nc -lk 9999 \n",
    "lines = ssc.socketTextStream('localhost', 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))#sequqnce of rdds\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "#mark 2 streams as outputstreams\n",
    "lines.pprint()#parallel print(action in rdd is blocking:drive program; output operation is not blocking)\n",
    "wordCounts.pprint()\n",
    "\n",
    "#non-blocking(create a new thread)\n",
    "ssc.start()  # Start the computation\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(20)  # Wait for the computation to terminate(blocking: pause the main program for 20 sec)\n",
    "ssc.stop(stopSparkContext=False)  # Stop the StreamingContext without stopping the SparkContext(don't want sc to be deleted: cannot rerun)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U7E4FOqub249",
    "outputId": "11b6e078-096b-4fd4-c4eb-a107bc4b04ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-17 16:59:45\n",
      "-------------------------------------------\n",
      "('other', 15486)\n",
      "('first', 10815)\n",
      "('many', 9773)\n",
      "('new', 6272)\n",
      "('system', 5063)\n",
      "('american', 4744)\n",
      "('several', 4545)\n",
      "('century', 4492)\n",
      "('same', 4394)\n",
      "('=', 4313)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-17 16:59:50\n",
      "-------------------------------------------\n",
      "('other', 15319)\n",
      "('first', 10709)\n",
      "('many', 9575)\n",
      "('new', 6242)\n",
      "('system', 5111)\n",
      "('american', 4777)\n",
      "('century', 4538)\n",
      "('several', 4515)\n",
      "('same', 4453)\n",
      "('=', 4361)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-17 16:59:55\n",
      "-------------------------------------------\n",
      "('other', 15346)\n",
      "('first', 10517)\n",
      "('many', 9706)\n",
      "('new', 6218)\n",
      "('system', 5266)\n",
      "('american', 4940)\n",
      "('several', 4615)\n",
      "('=', 4451)\n",
      "('century', 4438)\n",
      "('same', 4270)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-17 17:00:00\n",
      "-------------------------------------------\n",
      "('other', 15196)\n",
      "('first', 10617)\n",
      "('many', 9848)\n",
      "('new', 6289)\n",
      "('system', 5093)\n",
      "('american', 4824)\n",
      "('several', 4519)\n",
      "('century', 4493)\n",
      "('=', 4332)\n",
      "('same', 4260)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-05-17 17:00:05\n",
      "-------------------------------------------\n",
      "('other', 15091)\n",
      "('first', 10463)\n",
      "('many', 9703)\n",
      "('new', 6175)\n",
      "('system', 5102)\n",
      "('american', 4829)\n",
      "('several', 4523)\n",
      "('century', 4520)\n",
      "('=', 4369)\n",
      "('same', 4325)\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "\n",
    "# split the rdd into 5 equal-size parts\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 5 seconds\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Do word-counting as before\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use transform() to access any rdd transformations not directly available in SparkStreaming\n",
    "topWords = wordCounts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "topWords.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(25)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early radical',\n",
       " 'french revolution',\n",
       " 'pejorative way',\n",
       " 'violent means',\n",
       " 'positive label',\n",
       " 'self-defined anarchist',\n",
       " 'political philosophy',\n",
       " 'differ interpretation',\n",
       " 'relate movement',\n",
       " 'social movement']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fEjHmuH5b24-",
    "outputId": "4b01d770-4f6c-4d5f-b3c9-17dbc81e8e19",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-25 23:43:45\n",
      "-------------------------------------------\n",
      "(7890.0, 'great')\n",
      "(6180.0, 'popular')\n",
      "(5544.0, 'best')\n",
      "(4662.0, 'good')\n",
      "(4242.0, 'important')\n",
      "(2340.0, 'strong')\n",
      "(2322.0, 'greater')\n",
      "(2058.0, 'successful')\n",
      "(1850.0, 'novel')\n",
      "(1790.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-25 23:43:50\n",
      "-------------------------------------------\n",
      "(7578.0, 'great')\n",
      "(6126.0, 'popular')\n",
      "(5604.0, 'best')\n",
      "(4749.0, 'good')\n",
      "(4172.0, 'important')\n",
      "(2253.0, 'greater')\n",
      "(2252.0, 'strong')\n",
      "(2001.0, 'successful')\n",
      "(1948.0, 'novel')\n",
      "(1804.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-25 23:43:55\n",
      "-------------------------------------------\n",
      "(7893.0, 'great')\n",
      "(6009.0, 'popular')\n",
      "(5574.0, 'best')\n",
      "(4677.0, 'good')\n",
      "(4298.0, 'important')\n",
      "(2326.0, 'strong')\n",
      "(2172.0, 'greater')\n",
      "(1944.0, 'successful')\n",
      "(1868.0, 'novel')\n",
      "(1761.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-25 23:44:00\n",
      "-------------------------------------------\n",
      "(7776.0, 'great')\n",
      "(5925.0, 'popular')\n",
      "(5538.0, 'best')\n",
      "(4671.0, 'good')\n",
      "(4156.0, 'important')\n",
      "(2362.0, 'strong')\n",
      "(2205.0, 'greater')\n",
      "(1932.0, 'successful')\n",
      "(1892.0, 'novel')\n",
      "(1803.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-25 23:44:05\n",
      "-------------------------------------------\n",
      "(7566.0, 'great')\n",
      "(5916.0, 'popular')\n",
      "(5334.0, 'best')\n",
      "(4548.0, 'good')\n",
      "(4268.0, 'important')\n",
      "(2415.0, 'greater')\n",
      "(2314.0, 'strong')\n",
      "(2109.0, 'successful')\n",
      "(2018.0, 'novel')\n",
      "(1821.0, 'natural')\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "#do word count on Dstream\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/17 19:42:49 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:42:50 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'constructionism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'resistance'), ('earthly', 'consideration')]\n",
      "constructionism\n",
      "23/05/17 19:42:53 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:42:55 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'constructionism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n",
      "constructionism\n",
      "23/05/17 19:42:57 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:00 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'constructionism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n",
      "constructionism\n",
      "23/05/17 19:43:02 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:05 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'constructionism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n",
      "constructionism\n",
      "23/05/17 19:43:07 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:10 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:==================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realism/constructivism\n",
      "23/05/17 19:43:12 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:15 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'socialist-revolutionary'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n",
      "realism/constructivism\n",
      "23/05/17 19:43:17 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:20 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'patriarchates/patriarchs'), ('nonviolent', 'communication'), ('earthly', 'consideration')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:=====================>                                    (3 + 5) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realism/constructivism\n",
      "23/05/17 19:43:22 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:25 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'entertainers-turned-governor'), ('nonviolent', 'segregationist'), ('earthly', 'consideration')]\n",
      "realism/constructivism\n",
      "23/05/17 19:43:27 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:30 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'entertainers-turned-governor'), ('nonviolent', 'segregationist'), ('earthly', 'consideration')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 47:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realism/constructivism\n",
      "23/05/17 19:43:32 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:35 WARN QueueInputDStream: queueStream doesn't support checkpointing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 'realism/constructivism'), ('desirable', 'characteristic'), ('other', 'entertainers-turned-governor'), ('nonviolent', 'segregationist'), ('earthly', 'consideration')]\n",
      "realism/constructivism\n",
      "23/05/17 19:43:37 WARN QueueInputDStream: queueStream doesn't support checkpointing\n",
      "23/05/17 19:43:39 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 5)#5 is batch size\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 8\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', numPartitions)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunction(newValues,runningValue):\n",
    "    #newavlues:list of strings, runningValue: current longest string\n",
    "    if runningValue is None:\n",
    "        runningValue = \"\"\n",
    "    for value in newValues:#running values is empty/non empty\n",
    "        if len(value)>len(runningValue):\n",
    "            runningValue = value\n",
    "    return runningValue\n",
    "\n",
    "#clean the data\n",
    "cleaned_line = lines.map(lambda line: line.split()).filter(lambda line:len(line)==2)\n",
    "stateDstream = cleaned_line.updateStateByKey(updateFunction)#key is adj: same keys are updated together\n",
    "\n",
    "# Print the results\n",
    "stateDstream.foreachRDD(lambda rdd: print(rdd.take(5)))\n",
    "stateDstream.foreachRDD(lambda rdd: print(rdd.lookup(\"social\")[0]))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYd0f4ihb24_",
    "outputId": "3e2c8e3b-b8bb-4361-ef18-c82351b7fc47",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Stateful word count\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations(restore state from previous time)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)#10 batches, each batch is smaller\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "#newValues a list of size 1 for word count; runningCount is current count\n",
    "#if key doesn't exist in the state\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)#add new count to the running count; \n",
    "    #sum(list, addition value added to the final sum)\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "#other rdd(after every batch, therer is a new state)(how the state is updated after each batch, initial is empty)\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc)#same keys are grouped together\n",
    "\n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))\n",
    "\n",
    "def printResults(rdd):\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.take(5))\n",
    "    print('refinery:', rdd.lookup('refinery')[0])\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u06pj0bxb25A",
    "outputId": "332c25f8-cdfe-43a4-f286-201b23dfcef9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MG algorithm for approximate word count\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "k = 10000\n",
    "threshold = 0\n",
    "total_decrement = 0\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory.  Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    newValue = sum(newValues, runningCount) - threshold#reduce the size of the state\n",
    "    return newValue if newValue > 0 else None\n",
    "    # add the new values with the previous running count to get the new count\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b) \\\n",
    "                      .updateStateByKey(updateFunc)#stateful wordcount as before\n",
    "            \n",
    "counts_sorted = running_counts.transform(lambda rdd: rdd.sortBy(lambda x: x[1], False))# sort it based on the counts\n",
    "\n",
    "def printResults(rdd):\n",
    "    global threshold, total_decrement \n",
    "    rdd.cache()\n",
    "    print(\"Total distinct words: \", rdd.count())\n",
    "    print(rdd.map(lambda x: (x[0], x[1], x[1]+total_decrement)).take(5))#lower bound, upper bound[x[1], x[1]+total_decrement]\n",
    "    lower_bound = rdd.lookup('refinery')\n",
    "    if len(lower_bound) > 0:\n",
    "        lower_bound = lower_bound[0]\n",
    "    else:\n",
    "        lower_bound = 0\n",
    "    print('refinery:', lower_bound, ',', lower_bound + total_decrement)\n",
    "    if rdd.count() > k:#\n",
    "        threshold = rdd.zipWithIndex().map(lambda x: (x[1], x[0])).lookup(k)[0][1]#index will be the value, swap key value(index, count)max_count become new threshold\n",
    "    else:\n",
    "        threhold = 0\n",
    "    print(\"Next threshold = \", threshold)\n",
    "    total_decrement += threshold\n",
    "    rdd.unpersist()\n",
    "\n",
    "counts_sorted.foreachRDD(printResults)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(50)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dc-w55n1b25B",
    "outputId": "f7aabd91-e272-4e73-a561-e46a0e04bc6a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a queue of RDDs\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rdd = sc.parallelize([i, i, i, i, i])\n",
    "    rddQueue += [rdd]\n",
    "        \n",
    "# Create a StreamingContext with batch interval of 3 seconds\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# Feed the rdd queue to a DStream\n",
    "nums = ssc.queueStream(rddQueue)\n",
    "\n",
    "# Compute the sum over a sliding window of 9 seconds for every 3 seconds\n",
    "# slidingSum = nums.reduceByWindow(lambda x, y: x + y, None, 9, 3)\n",
    "slidingSum = nums.reduceByWindow(lambda x, y: x + y, lambda x, y: x - y, 9, 3) #function itself and inverse function;\n",
    "# sliding window size 3;wondow length is 9\n",
    "\n",
    "slidingSum.pprint()\n",
    "\n",
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(24)  # Wait for the computation to terminate\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WTt1BJP1b25B",
    "outputId": "afbe2556-2ea6-49ab-95d2-baef4687d9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 10:57:17 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "23/04/26 10:57:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/temporary-a70b4ed2-ecd1-4ca4-87f8-abc0bf9def01. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/26 10:57:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|happy|1    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|happy|2    |\n",
      "|best |1    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|best|2    |\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|happy|3    |\n",
      "+-----+-----+\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Word count using structured streaming: Complete mode vs update mode\n",
    "#spark streaming is incorporated with spark sql\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows(#covert multiple columns into rows) \n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "word_counts = words.groupBy('word').count()#very simple compared with previous task\n",
    "\n",
    "# Start running the query\n",
    "#.option('truncate', 'false')#default truncate to 20 rows\\\n",
    "#.trigger(processingTime='5 seconds') \\#trigger every 5 seconds; it's complete mode\n",
    "query = word_counts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('update')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .start()\n",
    "#.trigger(processingTime='5 seconds') \\\n",
    "# try update mode; append mode not supported\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2wkZDNFb25C",
    "outputId": "08a91650-ab93-4eb3-8b35-ebae0933f880"
   },
   "outputs": [],
   "source": [
    "# Append mode with selection condition\n",
    "# Note: complete mode not supported if no aggregation\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'))\n",
    "\n",
    "long_words = words.filter(length(words['word'])>=3)\n",
    "\n",
    "# Start running the query \n",
    "query = long_words\\\n",
    "        .writeStream\\\n",
    "        .outputMode('update')\\#cannot use append mode here\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('../data/adj_noun_pairs.txt', numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISq4yibib25C",
    "outputId": "b2ce8ab9-e791-4892-d237-a0bab7de7ce3"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\#keep timestamp\n",
    "        .load()\n",
    "        \n",
    "# Split the lines into words, retaining timestamps\n",
    "# split() splits each line into an array, and explode() turns the array into multiple rows\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'),\n",
    "                     lines.timestamp)\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 seconds\", \"5 seconds\"),\n",
    "    words.word)\\\n",
    "    .count()\n",
    "\n",
    "# Start running the query \n",
    "query = windowedCounts\\\n",
    "        .writeStream\\\n",
    "        .outputMode('update')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 23:51:25 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "DataFrame[word: string, timestamp: timestamp, word: string, sentiment: double]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'word' is ambiguous, could be: word, word.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/ipykernel_5169/808440479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Compute the sentiment score for each window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m windowed_sentiment_scores = sentiment_scores.groupBy(window(sentiment_scores.timestamp, '10 seconds', '5 seconds'),\\\n\u001b[0;32m---> 28\u001b[0;31m                                                      sentiment_scores[\"word\"])\\\n\u001b[0m\u001b[1;32m     29\u001b[0m                                             \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                             \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \"\"\"\n\u001b[1;32m   1964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Reference 'word' is ambiguous, could be: word, word."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnjcduqcb25D",
    "outputId": "c4c2b168-a3a3-4fa6-bd43-3f141ee2ecdf"
   },
   "outputs": [],
   "source": [
    "# Rate source (for testing) - Generates data at the specified number of rows per second, \n",
    "# each output row contains a timestamp and value. Where timestamp is a Timestamp type \n",
    "# containing the time of message dispatch, and value is of Long type containing the message count, \n",
    "# starting from 0 as the first row. This source is intended for testing and benchmarking.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "        \n",
    "# Start running the query \n",
    "query = streamingDf\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFe00e_qb25D",
    "outputId": "2f99b431-38d5-4cb2-8b7f-f256ec4e2d72"
   },
   "outputs": [],
   "source": [
    "# Stream–Static Joins\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "staticDf = spark.createDataFrame([(1, 'apple'), (2, 'orange'), (10, 'banana')], ['id', 'name'])\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(staticDf, streamingDf['value'] == staticDf['id'])\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "# also support leftOuter, but not rightOuter\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSqYNEKvb25E",
    "outputId": "d4c99e1e-dd58-4da5-c48c-5438a431d700"
   },
   "outputs": [],
   "source": [
    "# Stream-stream Joins\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.join(streamingDf2, 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhINcOD2b25E",
    "outputId": "fc92cef4-9b3e-4e63-d39c-f41abe922366"
   },
   "outputs": [],
   "source": [
    "# Stream–stream Joins with watemarking\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 4) \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "streamingDf = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .load()\n",
    "\n",
    "streamingDf2 = spark\\\n",
    "        .readStream\\\n",
    "        .format('rate')\\\n",
    "        .option('rowsPerSecond', 2) \\\n",
    "        .load()\n",
    "\n",
    "# Start running the query \n",
    "query = streamingDf.withWatermark('timestamp', '3 seconds').join(streamingDf2.withWatermark('timestamp', '3 seconds'), 'value')\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option('truncate', 'false')\\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination(20)\n",
    "query.stop()\n",
    "print(\"Finished\")\n",
    "\n",
    "#The current watermark is computed by looking at the MAX(eventTime) seen across all of the partitions in \n",
    "#the query minus a user specified delayThreshold. Due to the cost of coordinating this value across partitions,\n",
    "#the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. \n",
    "#In some cases we may still process records that arrive more than delayThreshold late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 10:30:24 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "DataFrame[word: string, timestamp: timestamp, sentiment: double]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": " Column window#39 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/ipykernel_914/2721358707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Compute the sentiment score for each window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m windowed_sentiment_scores = sentiment_scores.groupBy(window(sentiment_scores.timestamp, '10 seconds', '5 seconds'),\\\n\u001b[0m\u001b[1;32m     29\u001b[0m                                                      sentiment_scores.word)\\\n\u001b[1;32m     30\u001b[0m                                             \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mexprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.1-bin-hadoop2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Column window#39 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = spark.createDataFrame(sc.textFile(\"AFINN-111.txt\") \n",
    "                    .map(parse_line), ['word', 'sentiment']).cache()\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "\n",
    "# Split the lines into words and explode the array\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'), lines.timestamp)\n",
    "\n",
    "# Join the words with their sentiment scores\n",
    "sentiment_scores = words.join(word_sentiments, words.word == word_sentiments.word)\\\n",
    "                .select(words.word, words.timestamp,word_sentiments.sentiment)\n",
    "\n",
    "print(sentiment_scores)\n",
    "\n",
    "# Compute the sentiment score for each window\n",
    "windowed_sentiment_scores = sentiment_scores.groupBy(window(sentiment_scores.timestamp, '10 seconds', '5 seconds'),\\\n",
    "                                                     sentiment_scores.word)\\\n",
    "                                            .agg(sum(sentiment_scores.sentiment).alias('score'))\\\n",
    "                                            .orderBy('score', ascending=False)\n",
    "\n",
    "# Output the results in complete mode\n",
    "query = windowed_sentiment_scores.writeStream\\\n",
    "                                 .outputMode('complete')\\\n",
    "                                 .format('console')\\\n",
    "                                 .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 11:14:56 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "23/04/26 11:14:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/h0/cr6ykp052gngl6yqd4mtpvtm0000gn/T/temporary-f4c45395-daa8-4440-b58a-d7e0f4ba463e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/26 11:14:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+---------+\n",
      "|window|word|count|sentiment|\n",
      "+------+----+-----+---------+\n",
      "+------+----+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "|window                                    |word   |count|sentiment|\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "|{2023-04-26 11:15:05, 2023-04-26 11:15:15}|happy  |2    |6.0      |\n",
      "|{2023-04-26 11:15:00, 2023-04-26 11:15:10}|happy  |2    |6.0      |\n",
      "|{2023-04-26 11:15:05, 2023-04-26 11:15:15}|popular|1    |3.0      |\n",
      "|{2023-04-26 11:15:00, 2023-04-26 11:15:10}|popular|1    |3.0      |\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "|window                                    |word   |count|sentiment|\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "|{2023-04-26 11:15:05, 2023-04-26 11:15:15}|happy  |3    |9.0      |\n",
      "|{2023-04-26 11:15:00, 2023-04-26 11:15:10}|happy  |2    |6.0      |\n",
      "|{2023-04-26 11:15:05, 2023-04-26 11:15:15}|popular|1    |3.0      |\n",
      "|{2023-04-26 11:15:00, 2023-04-26 11:15:10}|popular|1    |3.0      |\n",
      "|{2023-04-26 11:15:10, 2023-04-26 11:15:20}|happy  |1    |3.0      |\n",
      "+------------------------------------------+-------+-----+---------+\n",
      "\n",
      "Finished\n",
      "23/04/26 12:09:58 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1410444 ms exceeds timeout 120000 ms\n",
      "23/04/26 12:09:58 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = spark.createDataFrame(sc.textFile(\"AFINN-111.txt\") \n",
    "                    .map(parse_line), ['word', 'sentiment']).cache()\n",
    "\n",
    "lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', '9999')\\\n",
    "        .option('includeTimestamp', 'true')\\\n",
    "        .load()\n",
    "\n",
    "# Split the lines into words and explode the array\n",
    "words = lines.select(explode(split(lines.value, ' ')).alias('word'), lines.timestamp)\n",
    "\n",
    "happy_words = words.groupBy(window(words.timestamp,\"10 seconds\", \"5 seconds\"),\n",
    "                           words.word)\\\n",
    "                           .count()\\\n",
    "                           .join(word_sentiments, words.word == word_sentiments.word)\\\n",
    "                           .select(col(\"window\"), words.word, col(\"count\"),col(\"sentiment\"))\\\n",
    "                           .withColumn('sentiment', col(\"count\")*col(\"sentiment\"))\\\n",
    "                           .orderBy(col(\"sentiment\").desc())\n",
    "\n",
    "# Output the results in complete mode\n",
    "query = happy_words.writeStream\\\n",
    "                    .outputMode('complete')\\\n",
    "                    .format('console')\\\n",
    "                    .option('truncate','false')\\\n",
    "                    .trigger(processingTime='5 seconds')\\\n",
    "                    .start()\n",
    "\n",
    "query.awaitTermination(25)\n",
    "query.stop()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-26 10:51:25\n",
      "-------------------------------------------\n",
      "(7890.0, 'great')\n",
      "(6180.0, 'popular')\n",
      "(5544.0, 'best')\n",
      "(4662.0, 'good')\n",
      "(4242.0, 'important')\n",
      "(2340.0, 'strong')\n",
      "(2322.0, 'greater')\n",
      "(2058.0, 'successful')\n",
      "(1850.0, 'novel')\n",
      "(1790.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-26 10:51:30\n",
      "-------------------------------------------\n",
      "(7578.0, 'great')\n",
      "(6126.0, 'popular')\n",
      "(5604.0, 'best')\n",
      "(4749.0, 'good')\n",
      "(4172.0, 'important')\n",
      "(2253.0, 'greater')\n",
      "(2252.0, 'strong')\n",
      "(2001.0, 'successful')\n",
      "(1948.0, 'novel')\n",
      "(1804.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-26 10:51:35\n",
      "-------------------------------------------\n",
      "(7893.0, 'great')\n",
      "(6009.0, 'popular')\n",
      "(5574.0, 'best')\n",
      "(4677.0, 'good')\n",
      "(4298.0, 'important')\n",
      "(2326.0, 'strong')\n",
      "(2172.0, 'greater')\n",
      "(1944.0, 'successful')\n",
      "(1868.0, 'novel')\n",
      "(1761.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-26 10:51:40\n",
      "-------------------------------------------\n",
      "(7776.0, 'great')\n",
      "(5925.0, 'popular')\n",
      "(5538.0, 'best')\n",
      "(4671.0, 'good')\n",
      "(4156.0, 'important')\n",
      "(2362.0, 'strong')\n",
      "(2205.0, 'greater')\n",
      "(1932.0, 'successful')\n",
      "(1892.0, 'novel')\n",
      "(1803.0, 'natural')\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-04-26 10:51:45\n",
      "-------------------------------------------\n",
      "(7566.0, 'great')\n",
      "(5916.0, 'popular')\n",
      "(5334.0, 'best')\n",
      "(4548.0, 'good')\n",
      "(4268.0, 'important')\n",
      "(2415.0, 'greater')\n",
      "(2314.0, 'strong')\n",
      "(2109.0, 'successful')\n",
      "(2018.0, 'novel')\n",
      "(1821.0, 'natural')\n",
      "...\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Find the most positive words in windows of 5 seconds from streaming data\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "def parse_line(l):\n",
    "    x = l.split(\"\\t\")\n",
    "    return (x[0], float(x[1]))\n",
    "\n",
    "word_sentiments = sc.textFile(\"AFINN-111.txt\") \\\n",
    "                    .map(parse_line).cache()\n",
    "    \n",
    "ssc = StreamingContext(sc, 5)\n",
    "rdd = sc.textFile('adj_noun_pairs.txt', 8)\n",
    "rddQueue = rdd.randomSplit([1,1,1,1,1], 123)\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "#do word count on Dstream\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                   .map(lambda word: (word, 1)) \\\n",
    "                   .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Determine the words with the highest sentiment values by joining the streaming RDD\n",
    "# with the static RDD inside the transform() method and then multiplying\n",
    "# the frequency of the words by its sentiment value\n",
    "happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n",
    "                            .map(lambda t:\n",
    "                                 (t[1][0] * t[1][1], t[0])) \\\n",
    "                            .transform(lambda rdd: rdd.sortByKey(False))\n",
    "\n",
    "happiest_words.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(25)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentiments = spark.createDataFrame(sc.textFile(\"../data/AFINN-111.txt\") \n",
    "                    .map(parse_line), ['word', 'sentiment']).cache()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
